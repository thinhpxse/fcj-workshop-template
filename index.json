[{"uri":"https://thinhpxse.github.io/fcj-workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Phan Xuan Thinh\nPhone Number: 0945645753\nEmail: thinhpxse184527@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: ‚ÄúCloud Day Vietnam 2025 : AI Edition‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nLet‚Äôs Architect! Learn About Machine Learning on AWS Machine Learning (ML) is becoming essential for organizations wanting to make data-driven decisions, automate processes, and build intelligent applications. ML models are not static ‚Äî they continuously improve as more data flows in, helping organizations adapt to business changes.\nThis blog highlights several AWS resources, architectures, workshops, and customer stories to help teams design, deploy, and scale ML workloads effectively.\nZero to Machine Learning: Jump-Start Your Data-Driven Journey A session from AWS re:Invent 2023 showing how organizations with limited resources (time, budget, expertise) can quickly start ML workloads on AWS.\nKey ideas:\nUse analytics + ML to build end-to-end data pipelines. Apply low-code / no-code tooling to accelerate ML adoption. Leverage AWS services to reduce operational overhead and shorten time-to-value. Introduction to MLOps Engineering on AWS MLOps extends DevOps practices to ML workloads and helps teams automate and manage the entire machine-learning lifecycle.\nCore concepts include:\nVersioning data, models, experiments. Automated training, testing, and deployment pipelines. Monitoring model performance after deployment. Ensuring reliability and scalability of ML systems. This section provides architecture patterns and best practices to adopt MLOps using AWS services.\nGenerative AI Infrastructure at Amazon An inside look at AWS Trainium and AWS Inferentia ‚Äî custom silicon built to optimize ML training and inference at scale.\nHighlights:\nReduce training costs for deep learning \u0026amp; generative AI models. Lower inference latency for production workloads. Improve performance for tasks like LLMs, computer vision, and recommendation systems. These accelerators help customers scale ML workloads efficiently.\nCustomer Stories Pinterest Pinterest shares how they:\nDesigned and orchestrated ML training environments. Ingested large-scale data into ML pipelines. Used containerized training jobs and distributed systems to speed up experimentation. Booking.com Booking.com shows how they use Amazon SageMaker to:\nBuild and train ML models. Analyze data and conduct online experimentation. Improve relevance ranking models and accelerate data-science iteration cycles. SageMaker Immersion Day Workshop A hands-on workshop demonstrating the full ML workflow on SageMaker:\nFeature engineering. Selecting algorithms and training models. Hyperparameter tuning. Deployment to production. Debugging and monitoring model behavior. The workshop simulates a near-real production ML scenario using end-to-end AWS services.\nConclusion This blog provides an overview of tools, architectures, and success stories for building ML solutions on AWS. With managed services, purpose-built hardware, and best-practice guides, AWS enables organizations to move ML models from experimentation into scalable, production-ready systems.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for official reports, including this notice.\nRevolutionizing Telecom Revenue Assurance: The AWS AI-Driven Framework for Next-Generation Solutions Revenue Assurance (RA) is critical in telecommunications‚Äîensuring accurate billing, validating usage data, reconciling partner settlements, and preventing revenue leakages. Traditional RA methods are often reactive, rely on partial sampling, and struggle to scale with modern telecom services such as 5G, IoT, and complex partner ecosystems. These limitations lead to annual revenue losses estimated between 1%‚Äì3% of total operator revenue.\nThis blog introduces the AWS AI-driven framework designed to modernize RA, enabling proactive detection, automated remediation, and large-scale intelligence across telecom operations.\nAWS AI/ML Framework for Modern Revenue Assurance AWS proposes a three-layer architecture to transform legacy RA platforms into intelligent, scalable systems:\n1. Foundation Layer Powered by Amazon SageMaker, this layer provides:\nHigh-performance ML infrastructure Custom model development and training Purpose-built accelerators Scalable data pipelines for petabyte-level telecom data 2. Middle Layer Amazon Bedrock offers:\nAccess to high-quality Foundation Models (FMs) Fine-tuning capabilities for telecom-specific tasks RAG (Retrieval-Augmented Generation) pipelines Enterprise-grade controls for governance and data security 3. Top Layer Amazon Q acts as:\nAn enterprise AI assistant A system for validating configurations, billing checks, and root-cause analysis A tool to generate summaries, explanations, and workflows for RA processes This layered architecture supports seamless ingestion, processing, and interpretation of large telecom datasets through zero-ETL integrations.\nGenerative AI Approaches for Revenue Assurance AWS integrates Generative AI and ML techniques to overcome the rigidity of rule-based RA systems.\nAI-driven capabilities include: Automated pattern recognition: Detect complex or emerging leak patterns across billing, usage, partner settlements, and mediation data Predictive modeling: Forecast potential leakage risks before they occur Intelligent process automation: Use agentic AI to autonomously validate, reconcile, and recommend corrective actions Explainable anomaly detection: Provide clear reasoning behind identified issues These capabilities result in faster detection, reduced manual work, and significantly improved accuracy.\nIndustry Use Cases \u0026amp; Business Impact 1. Launching new 5G services and network slicing Generative AI analyzes SLA structures, configuration rules, and usage correlations to ensure correctness.\nBenefits:\nUp to ~30% reduction in SLA-related revenue errors ~65% faster validation cycles 2. Partner revenue sharing \u0026amp; settlement AI reads contracts, compares commercial terms, identifies deviations, and accelerates settlement workflows.\nBenefits:\nReduced partner revenue leakage (~3%) 25% improvement in settlement accuracy 3. Real-time usage reconciliation AI models detect micro-leakage, event losses, and data inconsistencies in high-volume 5G/IoT workloads.\nBenefits:\nUp to 70% reduction in CDR-to-bill processing delay 4. Dynamic pricing and catalog assurance AI validates product configurations, pricing logic, and simulations before launch.\nBenefits:\n~35% reduction in revenue leakage from misconfigurations Faster time-to-market 5. Explainable detection with remediation recommendations AI-generated insights help analysts understand anomalies quickly and resolve issues faster.\nBenefits:\nAnalyst productivity improvement (~40%) 60% faster response time Recommended AWS Architecture Components A modern RA system requires integration across billing, CRM, mediation, network analytics, and data lake environments. AWS recommends:\nData Ingestion \u0026amp; Streaming Amazon MSK Amazon Kinesis Data Streams AWS Database Migration Service Secure file transfer \u0026amp; container-based ingestion Processing \u0026amp; Feature Engineering AWS Glue Amazon EMR AWS Lambda Feature Store Amazon Athena Storage \u0026amp; Advanced Analytics Amazon S3 Data Lake Amazon Redshift Amazon Aurora / Amazon RDS Vector databases (Amazon OpenSearch, Amazon Neptune, pgvector) ML/AI Deployment \u0026amp; Orchestration Amazon Bedrock Amazon SageMaker Amazon Q AWS Step Functions Amazon API Gateway Amazon CloudWatch for monitoring This modular architecture allows telecom operators to modernize step-by-step while ensuring scalability and resiliency.\nConclusion The AWS AI-driven framework represents a major shift in telecom Revenue Assurance‚Äîfrom reactive, rule-based methods to intelligent, proactive, automated systems. Using ML, Generative AI, and scalable cloud infrastructure, telecom operators can:\nMinimize revenue leakage Detect issues earlier and automate remediation Support new services like 5G and IoT with confidence Reduce operational costs and manual effort Ensure accurate billing, partner settlement, and product configuration This next-generation approach empowers telecom providers to innovate safely, protect profit margins, and deliver reliable services in an increasingly complex ecosystem.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference only. Please do not copy it verbatim into your official report, including this section.\nElectronic Arts Optimizes Game Update Processes with AWS The game patching process has always been a major challenge: game sizes can reach tens or even hundreds of gigabytes, comparing data on players‚Äô local machines is slow, and this often results in large patch sizes, long installation times, and poor user experience. This article introduces how Electronic Arts (EA) uses AWS to modernize its patching process, reduce update sizes, speed up downloads, and improve player experience.\nInitial Challenges: Slow and Inefficient Patching Traditional patching only compared files by name, so data moved between files was considered ‚Äúcompletely new,‚Äù forcing players to re-download many gigabytes of data. Large block sizes used for client-side comparison (e.g., 64 KB) reduced the ability to reuse old data, causing patches to grow in size. Patch computation happened on players‚Äô machines, taking many minutes or longer, especially for large games, negatively impacting user experience. The Solution: Known Version Patching (KVP) on AWS In August 2023, EA introduced Known Version Patching (KVP) ‚Äî a server-side patching system running entirely on AWS.\nHow KVP Works: Each game version is downloaded and unpacked on AWS, and hashes are calculated for every version. The system compares hash metadata between the new version and all previous versions to precisely identify changed data. Incremental patches are generated, containing only the modified data. A coordinator microservice receives events when a new version is available and triggers the patching pipeline and parallel processing jobs. Amazon Elastic Kubernetes Service (EKS) is used to run massive parallel jobs. Amazon Elastic File System (EFS) stores unpacked builds, enabling reuse and avoiding re-downloading or recalculating for older versions. Technical Architecture \u0026amp; Patching Workflow When a new version is available, a streaming system (e.g., Amazon MSK) sends notifications to trigger the pipeline. The coordinator service creates EKS jobs to download, extract, hash, compare metadata, and generate patches. Patches are uploaded to a CDN and distributed to players through the EA application. EFS caches unpacked builds, so future patches only compare changed parts ‚Äî saving time and compute resources. EKS uses CPU affinity, autoscaling, and optimized scheduling to maximize cluster throughput. Benefits and Improvements Achieved Smaller patch sizes: Patches were reduced by up to ~80% by including only changed data. Faster download and installation: Players can download and install patches up to 3.6√ó faster. Improved player experience: Less waiting time and reduced frustration during updates. Cost and resource optimization: Leveraging AWS infrastructure allows elastic scaling instead of relying on players‚Äô local machines. Faster development cycles: Automated patching significantly accelerates release and testing cycles. Conclusion By moving patch computation to AWS and applying the KVP system (EKS + EFS), EA achieved a major leap in game update efficiency: smaller, faster, more cost-effective, and more stable updates. Players receive patches more quickly, while EA reduces development time and infrastructure load.\nThis solution demonstrates the power of AWS in handling massive data workloads at global scale and delivering superior experiences for both developers and gamers.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference only. Please do not copy verbatim into official reports, including this notice.\nSecure Amazon Elastic VMware Service (Amazon EVS) with AWS Network Firewall Amazon EVS allows organizations to migrate, run, and scale VMware workloads natively on AWS, using VMware Cloud Foundation (VCF) deployed within an Amazon VPC on bare-metal Amazon EC2 instances. This approach enables rapid migration, seamless scaling, and minimal application refactoring.\n(aws.amazon.com)\nWhen building a hybrid cloud architecture integrating EVS, AWS VPCs, on-premises data centers, and internet connectivity, it is critical to implement consistent network security. Centralized inspection, unified firewall policy management, and consolidated logging/monitoring are essential. AWS recommends AWS Network Firewall as the core inspection and protection layer.\n(aws.amazon.com)\nThis blog explains how to secure an Amazon EVS environment using AWS Network Firewall, applying a centralized inspection model across EVS clusters, VPCs, on-premises environments, and the internet.\nArchitecture Overview AWS Network Firewall acts as a transparent ‚Äúbump-in-the-wire,‚Äù inserted via VPC or Transit Gateway routing, inspecting all traffic without requiring application-level changes. A typical architecture includes: EVS VPC, hosting the underlay VLAN subnets for VCF (management, vMotion, vSAN, NSX overlay, etc.). Workload VPCs (e.g., VPC01) where applications or services run. Ingress VPC with an Application Load Balancer for public traffic. Egress VPC with a NAT Gateway to centralize outbound internet access. Optional on-prem connectivity via AWS Direct Connect + Direct Connect Gateway.\n(aws.amazon.com) All traffic flows‚ÄîEVS ‚Üî VPCs, VPC ‚Üî VPC, VPC ‚Üî on-prem, and VPC ‚Üî Internet‚Äîare inspected by AWS Network Firewall, covering both east-west and north-south traffic.\nKey Benefits Centralized firewall control across EVS, AWS VPCs, on-premises, and internet paths Unified enforcement of security policies across complex hybrid environments Centralized logging and monitoring for audit, compliance, and troubleshooting Implementation Steps (Summary) Create an AWS Transit Gateway\nDisable default association/propagation. Create two route tables:\nPre-inspection Post-inspection\n(aws.amazon.com) Attach EVS, workload VPCs, ingress/egress VPCs, and optional Direct Connect Gateway\nUse the Pre-inspection route table for all attachments.\nDeploy AWS Network Firewall using Transit Gateway integration\nAWS automatically provisions required resources (firewall endpoints, internal routing, etc.).\nSimplifies configuration and reduces operational overhead.\nUpdate route tables\nPre-inspection: Default routing directs traffic into the Firewall for inspection Post-inspection: Routes inspected traffic back to the appropriate VPC destinations Inside each VPC: Update ingress/egress routes so all flows traverse the firewall\n(aws.amazon.com) (Optional) Configure firewall rule groups and logging\nStateful rules (e.g., HTTP/HTTPS, FQDN filtering, east-west segmentation) Send flow logs and alert logs to Amazon CloudWatch Logs or Amazon S3 When configured correctly, all network flows‚Äîacross EVS, AWS VPCs, on-prem, and the internet‚Äîare monitored and protected.\nAdvantages Over Traditional Security Approaches No application-level changes required; firewall operates transparently at the network layer Centralized management simplifies policy enforcement across multiple VPCs and VMware clusters Scales dynamically using AWS-managed infrastructure Unified logs streamline compliance, auditing, and incident analysis Conclusion This blog demonstrates how AWS Network Firewall can secure Amazon EVS by providing a centralized, scalable network inspection model. Traffic between EVS, workload VPCs, on-premises environments, and the internet is consistently monitored and enforced.\nOrganizations can migrate VMware workloads to AWS confidently‚Äîpreserving the benefits of cloud scalability while maintaining strong network security, unified control, and comprehensive visibility.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference only. Please do not copy verbatim into official reports, including this notice.\nBuilding an AI Gateway to Amazon Bedrock with Amazon API Gateway When building generative-AI applications at enterprise scale, organizations often need governance over foundation model usage ‚Äî such as authorization, quota management, tenant isolation, and cost control. This blog presents a reference pattern (originally implemented by a company) that uses Amazon API Gateway as the front layer to manage access to Amazon Bedrock services, providing enterprise-grade controls, while remaining transparent to client applications. :contentReference[oaicite:2]{index=2}\nAI Gateway Architecture Overview The reference architecture offers granular control over access to foundation models using fully managed AWS services; it\u0026rsquo;s transparent to clients and integrates seamlessly into existing enterprise environments. :contentReference[oaicite:3]{index=3}\nCore components:\nCustom domain routing (optional): using a DNS service (e.g. Amazon Route 53), so clients access the gateway via a company-specific domain rather than the default API Gateway domain. :contentReference[oaicite:5]{index=5} Amazon API Gateway: serves as the entry point for all calls to Bedrock ‚Äî handles authorization, request throttling / quota management, lifecycle controls, and integrates with security tools (e.g. WAF). :contentReference[oaicite:6]{index=6} Authorizer (e.g. AWS Lambda Authorizer): validates authentication tokens (like JWT) or integrates with existing identity systems (or other API-Gateway auth mechanisms) to enforce access control per tenant/user. :contentReference[oaicite:7]{index=7} Integration Lambda (request forwarder): receives requests from API Gateway, signs them with AWS credentials (AWS Signature v4), forwards them to the correct Amazon Bedrock endpoint ‚Äî preserving the original request details (parameters, action, etc.). This way, the gateway supports current and future Bedrock APIs without requiring updates to gateway code. :contentReference[oaicite:8]{index=8} Amazon Bedrock: provides foundation models and generative-AI capabilities behind the gateway. :contentReference[oaicite:9]{index=9} This pattern ensures clients can use standard SDKs (e.g. boto3) to call Bedrock as usual, but every request first passes through the gateway for governance. :contentReference[oaicite:10]{index=10}\nDeploying \u0026amp; Testing the Gateway The blog provides deployment instructions using infrastructure-as-code (e.g. via AWS CloudFormation). Initial deployment can disable authorization to simplify testing (e.g. private/regional endpoint, no JWT validation). :contentReference[oaicite:12]{index=12} After stack creation, you get outputs like GatewayUrl, VpcId, ApiId ‚Äî which you can use to test the gateway from inside the VPC (for example using a CloudShell environment). :contentReference[oaicite:13]{index=13} Example code is provided (e.g. Python + boto3) to create a client that works through the gateway. This client sets up custom headers (including service endpoint prefix) but signs request at the gateway side, thus preserving compatibility with Bedrock SDK calls. :contentReference[oaicite:14]{index=14} The blog shows how to call streaming APIs (e.g. for LLM inference) via gateway ‚Äî since Amazon API Gateway now supports response streaming, responses can be delivered to clients in real-time as the model generates them. :contentReference[oaicite:15]{index=15} Optionally, after basic validation, you can enable authorization (e.g. JWT token validation) by updating the CloudFormation stack parameter. Then re-deploy API Gateway to activate changes. :contentReference[oaicite:16]{index=16}\nAdditional Gateway Capabilities \u0026amp; Best Practices Using API Gateway plus this integration pattern gives you many enterprise-level controls:\nRate limiting \u0026amp; throttling / usage quotas: to control usage per tenant/user ‚Äî important for multi-tenant SaaS or shared internal platforms. :contentReference[oaicite:17]{index=17} Lifecycle management \u0026amp; canary releases: you can manage API versioning, gradually roll out new versions, use stage variables, and do safe deployments. :contentReference[oaicite:18]{index=18} Security hardening: integrate with AWS WAF to filter common web threats. :contentReference[oaicite:20]{index=20} Caching or prompt/response caching: for stable or repeated requests, to reduce cost and latency. :contentReference[oaicite:21]{index=21} Content filtering / custom validation: in the Lambda integration layer, you can implement additional checks ‚Äî e.g. filter sensitive content, sanitize inputs/outputs ‚Äî as a second line of defense beyond Bedrock‚Äôs built-in safety mechanisms. :contentReference[oaicite:22]{index=22} Benefits \u0026amp; Use-Cases This AI gateway pattern is particularly useful when you need:\nMulti-tenant access to LLM services ‚Äî each tenant gets per-tenant quota, isolation, usage tracking Governance and control over generative AI usage (authorization, cost control, rate limiting) Seamless integration with existing enterprise identity/auth systems (e.g. via JWT, Cognito, custom authorizers) Transparent SDK compatibility ‚Äî clients can still use Bedrock‚Äôs SDKs (e.g. boto3) as-is, without custom code changes Safe rollout and version control of AI APIs (canary deployments, staged rollout) In short ‚Äî you get enterprise-grade controls + flexibility + scalability + ease of adoption.\nConclusion The ‚ÄúAI Gateway to Amazon Bedrock with Amazon API Gateway‚Äù pattern offers a robust, scalable, and secure architecture for deploying generative-AI services in production. By putting a fully managed gateway in front of Bedrock, organizations gain fine-grained control over who can access what, how much, and when ‚Äî while preserving compatibility with existing tools and SDKs. It‚Äôs a powerful base for building multi-tenant AI services, internal AI-powered platforms, or scalable AI backends for customers.\nIf you want ‚Äî I can also generate a diagram (ASCII or markdown) of the architecture for easier inclusion in your documentation. ::contentReference[oaicite:23]{index=23}\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference only. Please do not copy it verbatim into official reports, including this notice.\nBuild Priority-Based Message Processing with Amazon MQ and AWS App Runner Many enterprise systems must process certain tasks immediately (such as urgent orders, system alerts, or mission-critical workflow events), while less important tasks can be processed later without affecting priority workloads. This blog explains how to build a priority-based message processing system using Amazon MQ, AWS App Runner, and Amazon DynamoDB ‚Äî ensuring high-priority messages are processed first, while low-priority events are still handled reliably in the background.\n(aws.amazon.com)\nSolution Overview The system uses several AWS fully managed services to create a scalable, cost-efficient, and reliable priority-based processing architecture:\nAmazon MQ as the message broker supporting JMS priority queues, ensuring high-priority messages are delivered and processed first. AWS App Runner as the fully managed service that runs the message-processing application with automatic scaling based on load. Amazon DynamoDB as a storage layer for message metadata and processing status, including real-time updates via DynamoDB Streams.\n(aws.amazon.com) This solution enables:\nMultiple priority levels: High, Standard, Low High-priority messages immediately move to the queue and bypass delays Standard and low-priority messages can be delayed or queued behind higher-priority traffic Real-time UI updates for message status tracking Two-layer retry and DLQ handling for durability and fault tolerance\n(aws.amazon.com) Priority Flow \u0026amp; Key Processing Mechanics Message intake \u0026amp; priority classification\nThe backend assigns JMS priorities (e.g., High = 9, Standard = 4, Low = 0). For Standard/Low messages, the application can apply a delay before enqueueing.\n(aws.amazon.com) Amazon MQ queueing\nMessages are stored in Amazon MQ queues that support priority ordering. The broker ensures high-priority messages always get delivered first. AWS App Runner processing\nApp Runner hosts a containerized message processor that automatically scales with throughput. Each message is processed and its status is stored in DynamoDB. Real-time monitoring via WebSockets\nDynamoDB Streams push status changes to the UI through WebSockets, enabling users to track message progress live.\n(aws.amazon.com) Retries \u0026amp; fault handling\nFailed messages are retried based on configured policies. Messages that continue to fail after retry limits can be moved to a dead-letter queue (DLQ).\n(aws.amazon.com) Benefits \u0026amp; Use Cases üîπ Key Benefits Guarantees that urgent, critical tasks are processed ahead of lower-priority work Fully managed architecture with minimal operational overhead Real-time updates improve user transparency and system observability Automatic scaling using App Runner ‚Äî no servers or capacity planning required Reliable event-driven design with retries and DLQ for durability\n(aws.amazon.com) üìå Ideal Use Cases Systems with mixed workloads: urgent vs. background tasks Order processing systems that include ‚Äúrush‚Äù or ‚ÄúVIP‚Äù orders Real-time monitoring dashboards that show progress of submitted requests Workflows that require predictable handling of priority traffic Applications where reliability and processing transparency matter Recommendations \u0026amp; Design Notes Configure Amazon MQ queues to fully support priority ordering; JMS priority must be enabled at the broker level. For production workloads, prefer native delay mechanisms of Amazon MQ (such as scheduled delivery or message TTL) rather than doing delay logic inside the application. Use IAM least privilege for App Runner, giving it only the permissions required for MQ and DynamoDB. Place Amazon MQ in private subnets for better security; avoid public accessibility if not needed.\n(aws.amazon.com) Conclusion By combining Amazon MQ + AWS App Runner + Amazon DynamoDB, you can build a scalable, fault-tolerant, and priority-aware message processing system.\nHigh-priority messages are processed quickly, lower-priority messages are handled efficiently in the background, and users get real-time visibility into the processing lifecycle.\nThis design is ideal for systems requiring:\nmulti-level message prioritization real-time message status updates high availability \u0026amp; fault tolerance minimum operational burden It offers strong reliability, clear priority control, and operational simplicity ‚Äî all powered by fully managed AWS services.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.1-week1/","title":"Week 1","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nReference Sources The summary content is based on the Explore AWS Services section of Cloud Journey - AWS Study Group.\nWeek 1 Week 1 Objectives Build AWS fundamentals: create an account, understand pricing and support mechanisms. Master basic concepts of access management (IAM), networking (VPC), and compute (EC2). Hands-on practice: create an account, configure IAM, deploy EC2, use Cloud9, host a static website on S3, and create a basic RDS database. Tasks to be completed this week: Day Task (Details) Start Date End Date Reference Source 2 Creating Your First AWS Account - Sign up for AWS Free Tier (create root user) - Enable MFA for root, add billing information, and create a billing alarm to avoid unexpected charges. TBD 08/09/2025 TBD 08/09/2025 cloudjourney.awsstudygroup.com 2 Managing Costs with AWS Budgets - Set up Budgets and alerts (email/SNS), use Cost Explorer, and practice creating a budget for a small project. TBD 09/09/2025 TBD 09/09/2025 cloudjourney.awsstudygroup.com 3 Getting Help with AWS Support - Understand support plans, how to create support cases, and use documentation \u0026amp; community (AWS Study Group) for debugging. TBD 10/09/2025 TBD 10/09/2025 cloudjourney.awsstudygroup.com 3 Access Management (IAM) - Create IAM users/groups, basic policies, practice the principle of least privilege, and create roles for services. TBD 10/09/2025 TBD 10/09/2025 cloudjourney.awsstudygroup.com 4 Networking Essentials (VPC) - Create a VPC, public/private subnets, route tables, Internet Gateway, NAT Gateway; understand CIDR, security groups vs NACLs. TBD 10/09/2025 TBD 10/09/2025 cloudjourney.awsstudygroup.com 4 Compute Essentials (EC2) - Launch an EC2 instance (AMI, instance type), configure SSH key pairs and security groups; manage EBS volumes and basic snapshots. TBD 11/09/2025 TBD 11/09/2025 cloudjourney.awsstudygroup.com 5 Instance Profiling with IAM Roles for EC2 - Create IAM roles for EC2 and attach policies so the instance can access S3/RDS/CloudWatch without access keys. TBD 11/09/2025 TBD 11/09/2025 cloudjourney.awsstudygroup.com 5 Cloud Development with AWS Cloud9 - Open the Cloud9 IDE, get familiar with the workspace, and deploy a small app (Node/Python) from Cloud9 to EC2 or S3. TBD 11/09/2025 TBD 11/09/2025 cloudjourney.awsstudygroup.com 6 Static Website Hosting with Amazon S3 - Create a bucket, configure static website hosting, set up public policies (or use CloudFront for security), and deploy a simple HTML/CSS site. TBD 12/09/2025 TBD 12/09/2025 cloudjourney.awsstudygroup.com 6 Database Essentials (Amazon RDS) - Create an RDS instance (MySQL/PostgreSQL), configure subnet groups, security groups, backup windows, and test connections from EC2. TBD 12/09/2025 TBD 12/09/2025 cloudjourney.awsstudygroup.com 7 Simplified Computing (Amazon Lightsail) - Explore how to use Lightsail to quickly deploy VMs or apps and compare costs and convenience with EC2. TBD 12/09/2025 TBD 12/09/2025 cloudjourney.awsstudygroup.com 7 Container Deployment with Amazon Lightsail Containers - Try deploying a simple container on Lightsail Containers to understand the workflow: image push, deployment, and public endpoint. TBD 12/09/2025 TBD 12/09/2025 cloudjourney.awsstudygroup.com Week 1 Results Created and secured the AWS account (MFA, billing alarms) and understood basic pricing and support. Understood IAM fundamentals and created test users/roles/policies. Set up a simple VPC, deployed EC2, connected via SSH, and tested EBS and snapshots. Used Cloud9 for rapid development and hosted a static website on S3. Created a basic RDS instance and tested connectivity; deployed a simple app on Lightsail and Lightsail Containers. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.2-week2/","title":"Week 2","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nWeek 2 Objectives Improve operational capabilities: autoscaling, monitoring, DNS, CDN, CLI, and NoSQL / caching databases. Practice deploying systems with basic scaling \u0026amp; high availability, using CloudWatch for monitoring and Route 53 for DNS configuration. Get familiar with DynamoDB, ElastiCache, CloudFront, Lambda@Edge, and basic Windows/Directory administration on AWS. Tasks to be carried out this week: Day Task (details) Start Date Completion Date Reference Material 2 Scaling Applications with EC2 Auto Scaling - Create Launch Template/Configuration, Auto Scaling Group (ASG), scale-out/scale-in policies; test by simulating increased load. TBD TBD cloudjourney.awsstudygroup.com. 2 Monitoring with Amazon CloudWatch - Create CloudWatch Alarm, custom metrics, log group; configure dashboard to monitor CPU, network, health check. TBD TBD cloudjourney.awsstudygroup.com. 3 Hybrid DNS Management with Amazon Route 53 - Create hosted zone, record sets (A/CNAME/ALIAS), health check; combine internal DNS with public DNS if needed. TBD TBD cloudjourney.awsstudygroup.com. 3 Command Line Operations with AWS CLI - Install \u0026amp; configure AWS CLI, practice EC2/S3/RDS/DynamoDB commands, small scripts to automate frequent tasks. TBD TBD cloudjourney.awsstudygroup.com. 4 NoSQL Database Essentials (Amazon DynamoDB) - Create table, design partition key \u0026amp; sort key, read/write items, test throughput (read/write capacity / on-demand). TBD TBD cloudjourney.awsstudygroup.com. 4 In-Memory Caching with Amazon ElastiCache - Deploy Redis/Memcached; integrate caching for applications; configure cluster replication (if needed) and test reduced latency. TBD TBD cloudjourney.awsstudygroup.com. 5 Networking on AWS Workshop - Advanced exercises on VPC, peering, overview of VPN/Direct Connect, review flow logs and network troubleshooting. TBD TBD cloudjourney.awsstudygroup.com. 5 Content Delivery with Amazon CloudFront - Create distribution, origin (S3/ALB), invalidation, verify TTL; use CloudFront to accelerate static assets. TBD TBD cloudjourney.awsstudygroup.com. 6 Edge Computing with CloudFront and Lambda@Edge - Write small Lambda@Edge functions to modify headers/redirect; understand edge compute use cases. TBD TBD cloudjourney.awsstudygroup.com. 6 Windows Workloads on AWS - Deploy EC2 Windows instance, license model, RDP access, basic patching \u0026amp; backup configuration. TBD TBD cloudjourney.awsstudygroup.com. 7 Directory Services (AWS Managed Microsoft AD) - Initialize AWS Managed Microsoft AD, domain join EC2 Windows, test authentication and simple group policy. TBD TBD cloudjourney.awsstudygroup.com. 7 Building Highly Available Web Applications - Design multi-AZ for EC2/RDS, load balancing (ALB), health checks, automatic scaling with ASG, use CloudFront + Route 53 to enhance HA and performance. TBD TBD cloudjourney.awsstudygroup.com. Week 2 Outcomes: Understand and configure Auto Scaling for EC2 applications; able to simulate scale-out/scale-in. Set up CloudWatch metrics/alarms and dashboards for basic monitoring; know how to collect logs. Configure DNS with Route 53, use CloudFront to optimize content delivery; experiment with simple Lambda@Edge functions. Deploy DynamoDB and ElastiCache; understand trade-offs between RDBMS / NoSQL / cache. Perform basic Windows workload administration and test AWS Managed Microsoft AD for authentication. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.3-week3/","title":"Week 3","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: This is a reference template. Do not copy it verbatim for any official report submission.\nObjectives for this week: Understand an overview of the services that support Migration on AWS. Learn the workflow for migrating systems from On-premises or another Cloud to AWS. Know the roles of each Migration service (DMS, SCT, VM Import/Export, Elastic Disaster Recovery‚Ä¶). Prepare fundamentals for next week‚Äôs Migration hands-on exercises. Tasks to be carried out this week: Day Task Start Date Completion Date References 2 - Read an overview of the Migrate to AWS category - Identify the key groups of Migration services dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about VM Migration with AWS VM Import/Export + How to import VMs + Supported formats dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 4 - Study Database Migration with: + AWS Database Migration Service (DMS) + Schema Conversion Tool (SCT) dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 5 - Read and take notes on AWS Elastic Disaster Recovery (DR) + How it works + Mechanisms for reducing downtime dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 6 - Research the ‚ÄúComing Soon‚Äù services: + AWS Migration Hub + AWS Application Migration Service + AWS Migration Evaluator + AWS Application Discovery Service dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ Achievements: Gained an understanding of the entire set of services in the Migrate to AWS group, including:\nAWS VM Import/Export AWS Database Migration Service (DMS) Schema Conversion Tool (SCT) AWS Elastic Disaster Recovery AWS Application Migration‚Äìrelated services (Coming Soon) Clearly understood the migration categories:\nVM Migration: Moving virtual machines. Database Migration: Migrating databases + converting schemas. Disaster Recovery: Preparing systems for recovery after incidents. Full Application Migration: (upcoming services). Understood the purpose of each service:\nVM Import/Export ‚Üí Move VMs from VMware/Hyper-V to EC2. DMS ‚Üí Migrate data with minimal downtime. SCT ‚Üí Convert schemas when changing engines (Oracle ‚Üí Aurora‚Ä¶). Elastic Disaster Recovery ‚Üí Create ready-to-failover replicas. Migration Hub ‚Üí Track the entire Migration project. Application Migration Service ‚Üí Automatically convert On-prem machines to EC2. Took notes on the use cases of each service and the differences between Migration \u0026amp; Disaster Recovery.\nPrepared materials for next week‚Äôs Migration practice sessions.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.4-week4/","title":"Week 4 ","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: This is only a reference template. Do not copy it verbatim for your official submission.\nObjectives for this week: Understand system optimization methods when operating on AWS. Grasp the key Operations tool groups: Monitoring, Automation, IaC, Resource Optimization, Cost Management. Practice with CloudWatch, Systems Manager, IAM, Resource Tags, and Amazon EBS automation. Be ready to optimize performance, cost, and security in real AWS environments. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Read the overview of Optimizing the System on AWS - Explain the role of Operations in AWS dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn Serverless Automation with AWS Lambda - Advanced Monitoring with CloudWatch \u0026amp; Grafana dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 4 - CloudWatch Advanced Workshop (Logs, Metrics, Alarms) - Manage resources with Tags and Resource Groups dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 5 - IAM \u0026amp; Resource Tags for fine-grained access control - Systems Management with AWS Systems Manager - Server access via Session Manager dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn Infrastructure as Code with AWS CloudFormation - Learn AWS CDK (Essentials + Advanced) - Resource Optimization with EC2 dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 7 - Network Monitoring with VPC Flow Logs - Cost \u0026amp; Usage Management - EBS Snapshot Automation with Amazon EBS DLM - Anomaly Detection for EBS Backups dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ Achievements: Understood the role of Operations in optimizing systems:\nAutomation (Lambda, Systems Manager) Advanced Monitoring (CloudWatch, Grafana) Standardized resource management using Tags Infrastructure as Code (CloudFormation, CDK) Cost \u0026amp; performance optimization (EC2 right-sizing, billing tools) Reliability \u0026amp; backup optimization (EBS, DLM, Snapshot Automation) Gained detailed understanding of each key service this week:\nAWS Lambda: Automations, event-driven tasks CloudWatch \u0026amp; Grafana: Dashboards, metrics, logs, anomaly detection IAM + Resource Tags: Fine-grained permissions Systems Manager + Session Manager: Server access \u0026amp; management without SSH CloudFormation \u0026amp; AWS CDK: Repeatable, version-controlled infrastructure deployment EC2 Resource Optimization: Right-sizing, unused instance checks VPC Flow Logs: Network traffic monitoring for security \u0026amp; debugging Cost and Usage Management: Cost tracking \u0026amp; alerts Amazon EBS Lifecycle Manager: Automated scheduled snapshots Completed notes and analysis on system optimization:\nPerformance optimization Cost optimization Security optimization Administration \u0026amp; automation optimization Prepared foundational knowledge for next week: Security Optimization / Performance Optimization / Cost Optimization.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.5-week5/","title":"Week 5","tags":[],"description":"","content":"‚ö†Ô∏è Note: This is only a sample template. Do not submit it as your official assignment.\nWeekly Objectives Understand AWS security principles: Identity, Access, Data Protection, Threat Detection. Learn the roles of IAM, KMS, VPC, Firewall Manager, GuardDuty, and other security services. Practice access control, data protection, identity management, and threat detection. Build foundational knowledge of AWS Security Best Practices for real-world environments. Tasks for This Week Day Tasks Start Date End Date References 2 - Overview of AWS Security - Identify 4 main areas: Identity, Access Control, Data Protection, Threat Detection dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 3 - Identity Federation using IAM Identity Center (AWS SSO) - IAM Permission Boundaries dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 4 - IAM Policies \u0026amp; Conditions - Security \u0026amp; Compliance with AWS Security Hub dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 5 - Private S3 access via VPC Endpoints - Application protection with AWS WAF dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 6 - Encryption using AWS KMS - Data protection using Amazon Macie - Secrets management with AWS Secrets Manager dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 7 - Centralized policy governance with Firewall Manager - Threat detection using AWS GuardDuty - Automated OS patching with EC2 Image Builder dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 8 - Cross-domain authentication with Amazon Cognito - Summary of S3 Security Best Practices dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com Achievements üîê Identity \u0026amp; Access Management IAM Identity Center (AWS SSO): Centralized identity management supporting SAML/OIDC. Permission Boundaries: Define the maximum permissions an entity can receive. IAM Policies \u0026amp; Conditions: Context-based access control using IP, MFA, time, etc. Amazon Cognito: User authentication with OAuth2, IdPs, and social login. üîí Data Protection AWS KMS: Data encryption, key management, key rotation. Amazon Macie: Detect sensitive data (PII) stored in S3. S3 Best Practices: Block Public Access, SSE-KMS, and safe Bucket Policies. üõ° Application \u0026amp; Network Security AWS WAF: Protects against SQL injection, XSS, bots, and rate-based attacks. Firewall Manager: Centralized management of WAF, Shield, and security policies. VPC Endpoints: Private S3 access without traversing the Internet. üïµ Threat Detection \u0026amp; Monitoring AWS GuardDuty: Detects anomalies and potential intrusions. AWS Security Hub: Compliance checks against CIS, PCI-DSS; unified security insights. ‚ôª System Patching EC2 Image Builder: Automated AMI patching to reduce vulnerabilities. Weekly Summary Gained a solid understanding of key AWS security services. Learned Identity Federation, Permission Boundaries, and advanced IAM controls. Understood encryption, data monitoring, and application security tools. Can design an AWS environment following security best practices. Ready for more advanced secure-by-design AWS implementations in upcoming weeks. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.6-week6/","title":"Week 6","tags":[],"description":"","content":"‚ö†Ô∏è Note: This is only a sample template. Do not submit it as your official assignment.\nWeekly Objectives Understand the Reliability pillar of the AWS Well-Architected Framework. Learn how to design resilient systems: backup, failover, high availability, and network reliability. Practice implementing reliability solutions: Backup, Multi-Attach, SQS/SNS, Clustering. Recognize High Availability models for Databases and Windows Server on AWS. Tasks for This Week Day Tasks Start Date End Date References 2 - Overview of AWS Reliability - Principles: Fault Tolerance, High Availability, Backup, Failover dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 3 - AWS Backup: configure backup plan, vault, and backup rules - Learn data protection strategies dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 4 - Network Reliability: VPC Peering - Centralized network management with Transit Gateway dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 5 - Messaging Systems: SQS (queue), SNS (pub-sub) - Applying messaging for distributed architectures dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 6 - Shared storage using Amazon EBS Multi-Attach - Database HA with Multi-Attach + Systems Manager dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 7 - Windows Server Failover Clustering (WSFC) on AWS - Advanced failover architectures dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 8 - SQL Server High Availability on AWS: + Version 2019 + Version 2022 dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com Achievements üõ° Data Protection Learned how to use AWS Backup for centralized backup management. Configured backup plans, vaults, lifecycle policies, and cross-region backup. Compared AWS Backup with manual snapshots. üåê Network Reliability VPC Peering: low-latency, private connection between VPCs without traversing the Internet. AWS Transit Gateway: manage hundreds of VPCs and on-prem networks using a hub-and-spoke model. Gained knowledge on designing highly available network architectures. üì© Messaging for Reliability Amazon SQS: asynchronous processing, load decoupling, preventing message loss. Amazon SNS: publish-subscribe model, fan-out architecture for microservices. Applied messaging patterns to build resilient and decoupled systems. üíæ Storage Reliability EBS Multi-Attach: share a single EBS volume across multiple EC2 instances ‚Üí improved HA for shared-disk applications. Used Systems Manager automation to configure database HA. ü™ü Windows \u0026amp; SQL Server High Availability Windows Server Failover Clustering (WSFC) on AWS: shared storage, automated failover nodes. SQL Server HA (2019 \u0026amp; 2022) on AWS: Always On Availability Groups Multi-AZ deployment Automatic failover Weekly Summary Gained a complete understanding of how to improve AWS infrastructure reliability across storage, networking, and messaging layers. Learned to implement High Availability for Windows Server and SQL Server workloads. Built strong foundational knowledge for designing AWS architectures aligned with the Reliability pillar of the Well-Architected Framework. Ready to apply HA/DR models in real-world Production environments. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.7-week7/","title":"Week 7","tags":[],"description":"","content":"Worklog Week 7 ‚Äì AWS Performance ‚ö†Ô∏è Note: This is only a sample template. Do not submit it as your official assignment.\nWeekly Objectives Understand system performance optimization techniques on AWS. Learn Docker, ECS, CI/CD, and workflow orchestration. Practice DevOps models for automated build, deploy, and scaling. Understand storage performance, hybrid storage, and NoSQL processing with DynamoDB. Tasks for This Week Day Tasks Start Date End Date References 2 - Overview of AWS Performance Optimization - The role of containerization \u0026amp; orchestration dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 3 - Learn Containerization with Docker - Build Dockerfile \u0026amp; follow best practices dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 4 - Amazon ECS: cluster, task, service - Compare EC2 mode vs Fargate mode dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 5 - Infrastructure as Code for ECS with AWS CDK - Auto-generate ECS Service, Task, Load Balancer dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 6 - CI/CD with AWS CodePipeline - Automate build ‚Üí test ‚Üí deploy - DevOps pipeline with CodePipeline dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 7 - Hybrid storage using AWS Storage Gateway - Windows File Storage using Amazon FSx dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com 8 - DynamoDB Advanced Applications - Workflow orchestration with AWS Step Functions - Storage Performance Workshop dd/mm/2025 dd/mm/2025 cloudjourney.awsstudygroup.com Achievements üê≥ Containerization \u0026amp; Orchestration Strong understanding of Docker: image building, layer optimization, registry operations. Mastered Amazon ECS components: Task Definitions Service Auto Scaling ECS Cluster (Fargate \u0026amp; EC2 mode) Learned how to deploy high-performance container workloads on AWS. üß± Infrastructure as Code for ECS Used AWS CDK to define ECS Services, Load Balancers, and Task Definitions. Improved configuration management through IaC with reproducible deployments. üîÑ CI/CD \u0026amp; DevOps Automation Automated deployments using AWS CodePipeline and CodeBuild. Built a fully automated pipeline: Source ‚Üí Build ‚Üí Test ‚Üí Deploy Applied DevOps principles for ECS workloads. üíæ Storage Performance AWS Storage Gateway: enhanced hybrid storage and caching. Amazon FSx: high-performance SMB storage for Windows workloads. Hands-on workshop on throughput, IOPS, and storage scaling. ‚ö° High Performance NoSQL Amazon DynamoDB: High-performance key-value \u0026amp; document store Adaptive capacity On-demand vs provisioned modes Global Tables for global-scale performance üîÅ Workflow Orchestration Built stateful workflows with AWS Step Functions. Orchestrated multiple AWS services in a high-performance pipeline. Weekly Summary Gained deep knowledge in performance optimization for containers, CI/CD, storage, and NoSQL. Learned to apply DevOps with Docker + ECS + CDK + CodePipeline. Understood performance scaling strategies in distributed systems. Ready to deploy high-performance, containerized workloads in AWS Production environments. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.8-week8/","title":"Week 8","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: This is a reference template only ‚Äî do not copy verbatim for official submission.\nObjectives of This Week: Understand cost optimization principles in the AWS Well-Architected Framework. Identify long-term cost reduction options: Savings Plans and Reserved Instances (RI). Visualize and analyze costs using Cost Explorer and Cost and Usage Reports (CUR). Learn to use AWS Glue + Amazon Athena for advanced cost data analysis. Tasks to Perform This Week: Day Tasks Start Date End Date Reference 2 - Overview of AWS Cost Optimization - Five cost optimization principles in AWS dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn Savings Plans and Reserved Instances (RI) - Compare Compute Savings Plans vs EC2 RI dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 4 - Cost visualization with AWS Cost Explorer - Cost and Usage Reports (CUR) dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 5 - Cost analysis using charts, trends, and tag-based cost allocation dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 6 - Build ETL pipeline with AWS Glue for Cost and Usage Reports dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 7 - Query cost data using Amazon Athena - Build advanced cost analysis dashboards dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ Achievements: üí∞ 1. Cost Savings Understood the differences between: Savings Plans: Flexible, apply to multiple services (EC2, Fargate, Lambda). Reserved Instances (RI): Cheaper but tied to specific instance type and region. Learned how to choose 1-year or 3-year Savings Plans based on commitment level. üìä 2. Cost Visualization \u0026amp; Analytics Used Cost Explorer to:\nView daily/monthly spending. Break down costs by service, tag, and account. Detect cost spikes and anomalies. Created Cost and Usage Reports (CUR) to track detailed billing data.\nüîç 3. Cost Data Analysis with Glue \u0026amp; Athena Created Glue Crawlers to index CUR data.\nQueried CUR data with Athena to:\nAnalyze costs by tag/product. Calculate total spending for EC2, S3, RDS, etc. Build custom reports as needed. Understood how to use Athena + Glue as a mini BI system for real-time cost analysis.\nWeekly Summary: Gained a deep understanding of AWS cost optimization from basic to advanced. Learned to use Savings Plans and Reserved Instances for long-term optimization. Became proficient with Cost Explorer, CUR, AWS Glue, and Amazon Athena for cost analysis. Able to build cost analytics dashboards to help organizations optimize cloud budgets. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.9-week9/","title":"Week 9","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: This is a reference template ‚Äî do not copy it verbatim for official submission.\nObjectives of This Week: Understand application modernization using Microservices, Serverless, and Event-driven architectures. Learn how to migrate from Monolith ‚Üí Microservices and set up CI/CD for modern applications. Study and practice Serverless series (DevAx, Book Store, Document Management System). Gain knowledge of API Gateway, SAM, Cognito, AppSync, CloudFront, Amplify, DynamoDB in modern architectures. Tasks to Perform This Week: Day Tasks Start Date End Date Reference 2 - Overview Application Modernization on AWS - Monolith to Microservices Migration dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 3 - CI/CD for Application Release - Building Microservices - Data Restructuring \u0026amp; Workflow Restructuring dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 4 - Event-Driven Architecture - SPA Authentication (Cognito) - AWS AI Services Integration dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 5 - Serverless - DevAx Series (Backend \u0026amp; Architecture) dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 6 - Serverless Book Store Series: Lambda + S3 + DynamoDB - Frontend for Serverless APIs - Deployment Automation with AWS SAM dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 7 - Cognito Authentication - Custom Domains + SSL - Event Processing with SQS/SNS - CI/CD for Serverless Applications dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 8 - Monitoring Serverless Apps - AppSync GraphQL APIs - Serverless Document Management System Series dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 9 - Document CRUD with Lambda + DynamoDB - Amplify Storage/Auth - API Gateway Frontend Integration dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 10 - Deployment with AWS SAM - CloudFront setup - Adding Search functionality dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 11 - DevOps for Document System - Distributed Tracing: X-Ray + CloudWatch dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ Achievements: üèó 1. Application Modernization Fundamentals Mastered the Monolith ‚Üí Microservices model. Understood how to refactor data, workflows, and architecture when migrating to microservices. Learned how to build CI/CD pipelines for fast and reliable releases. ‚ö° 2. Event-Driven \u0026amp; Serverless Architecture Applied event-driven patterns using S3, SNS, SQS, Lambda, and EventBridge. Implemented SPA Authentication with Amazon Cognito (User Pool, Identity Pool). Integrated AWS AI Services into applications (Rekognition, Comprehend, etc.). üü£ Serverless ‚Äì DevAx Series Serverless backend architecture using Lambda, API Gateway, and DynamoDB. Built scalable applications without server management. Optimized performance and error handling using event-driven patterns. üü¢ Serverless ‚Äì Book Store Series Backend Used Lambda + S3 + DynamoDB for CRUD operations and business logic. Routed API requests from API Gateway to Lambda. Frontend Built UI to call serverless APIs. Optimized state management, fetch, and re-rendering. Deployment Deployed with AWS SAM: template.yaml, automated build \u0026amp; deploy. Built complete CI/CD pipeline for serverless applications. Authentication Used Amazon Cognito for sign-in, sign-up, and JWT tokens. Advanced Configured custom domains + SSL using ACM + API Gateway. Implemented event processing with SNS \u0026amp; SQS. Monitored using CloudWatch: logs, metrics, and alarms. Built GraphQL APIs with AWS AppSync. üîµ Serverless ‚Äì Document Management System Series Backend \u0026amp; CRUD Implemented document CRUD with Lambda + DynamoDB. Optimized partition keys and GSIs. Storage \u0026amp; Auth Used AWS Amplify for Authentication, Storage, and Hosting. Secured document upload and download. Integration Connected frontend to backend using API Gateway. Distributed content quickly and securely with Cloud "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"\rNote: The information below is for reference purposes only, please do not copy verbatim for your report including this warning.\nDevteria Game Store Platform AWS Cloud E-commerce Solution for Digital Game Distribution Project Documentation üìÑ Devteria Shop Game Project With AWS (Word)\n1. Executive Summary Devteria Game Store is a scalable e-commerce platform for digital game licensing. Built on AWS, it delivers secure authentication, real-time inventory, automated order processing, and global content delivery. Supports thousands of concurrent users with 99.9% uptime and cost efficiency through serverless architecture.\n2. Problem Statement Current Challenges:\nTraditional stores struggle with traffic spikes Complex auth/payment reduces conversions Manual inventory causes overselling Lack of real-time analytics High infrastructure costs for peak capacity Solution: Devteria leverages AWS: CloudFront + S3 (fast delivery), Cognito (secure auth), API Gateway + Lambda (serverless backend), RDS + S3 (reliable storage), SQS + SNS (async processing), CodePipeline (CI/CD).\n3. Solution Architecture Core Components:\nFrontend: CloudFront CDN + S3 (React app, global cache, \u0026lt;2s load) Backend: API Gateway + Lambda (auto-scaling logic) + ALB + EC2 (microservices) Data: RDS PostgreSQL (users, catalog, orders) + S3 (game files, assets) + SQS/SNS (async processing) Security: Cognito (auth with MFA) + IAM (access control) + CloudWatch (monitoring) CI/CD: GitLab CodePipeline CodeBuild Deploy User Flow: Access site Login (Cognito) Browse games (API/Lambda/RDS) Add to cart Checkout License generation (SQS) Email (SNS) Secure download (S3)\n4. AWS Services Service Purpose Configuration CloudFront CDN 10M requests, 50GB transfer S3 Storage 100GB (frontend + assets) API Gateway API Management 1M requests/month Lambda Serverless Compute 5M invocations, 512MB EC2 Microservices 2x t3.medium RDS Database db.t3.small Multi-AZ ALB Load Balancer 1 ALB Cognito Authentication 10K users SQS + SNS Queue + Notifications 5M + 100K messages CloudWatch Monitoring Metrics + logs CodePipeline CI/CD 1 pipeline 5. Implementation Timeline (6 months) Month Milestones 1 Infrastructure: Setup AWS, VPC, RDS, S3, Cognito 2-3 Backend: Lambda APIs (auth, catalog, orders) + API Gateway 3 Frontend: React/Next.js app + Cognito integration 4 Advanced: Payment gateway + Admin dashboard + CI/CD 5 Testing: Load tests + Security audits + Performance tuning 6 Launch: Beta release Public launch 6. Budget Estimate Monthly Cost (10K users, 1K orders/month): ~$228\nService Cost CloudFront + S3 + API Gateway + Lambda $32 EC2 (2x t3.medium) + RDS (t3.small) $110 ALB + Cognito $50 SQS + SNS + CloudWatch + Other $36 Scaling: 50K users ($650/month), 100K users ($1,200/month)\nOne-time: Development ($5K-8K), Domain ($15/year), SSL (Free via ACM)\n7. Risk Assessment Risk Mitigation DDoS attacks AWS Shield, CloudFront, rate limiting Data breaches Encryption, IAM, regular audits Payment fraud 3D Secure, fraud detection Lambda cold starts Provisioned concurrency Cost overruns Budget alerts, auto-scaling limits Contingency: RDS automated backups, Multi-AZ deployment, CodePipeline rollback, static maintenance page\n8. Expected Outcomes Technical:\nPerformance: \u0026lt;2s page load globally Scalability: Handle 10x traffic spikes Reliability: 99.9% uptime Security: Zero breaches, PCI-ready Business:\n40% reduction in cart abandonment 60% less infrastructure management time 25-35% revenue increase from better UX Global market reach via CDN Long-term: Scale to 100K+ users, team gains AWS expertise, reusable microservices, rapid feature development\nNext Steps Proposal approval AWS account setup Team assembly Start Phase 1 Weekly progress reviews "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/","title":"Translated AWS Blog","tags":[],"description":"","content":"This section contains a list of AWS blogs that have been summarized and translated.\nBlog 1 - Learn About Machine Learning on AWS This blog introduces how AWS helps organizations build and operate Machine Learning workloads from basic to advanced levels. It covers the ‚ÄúZero to ML‚Äù journey, MLOps concepts for managing ML lifecycles, and purpose-built infrastructure such as AWS Trainium and Inferentia. The article also highlights real-world case studies from Pinterest and Booking.com, as well as the SageMaker Immersion Day workshop that demonstrates end-to-end ML implementation in near-production environments.\nBlog 2 - Revolutionizing Telecom Revenue Assurance with AWS AI-Driven Framework This blog explains how AWS applies AI/ML and Generative AI to modernize Revenue Assurance (RA) systems in the telecommunications industry. It focuses on a three-layer architecture (Amazon SageMaker, Amazon Bedrock, and Amazon Q) and highlights intelligent revenue-leakage detection, predictive risk analysis, and automated remediation. Real-world use cases include 5G services, partner settlements, and real-time usage reconciliation.\nBlog 3 - Electronic Arts Streamlines Game Patching with AWS This article describes how Electronic Arts (EA) built a Known Version Patching (KVP) system on AWS to optimize game update delivery. The solution leverages Amazon EKS and Amazon EFS to precompute patches on the server side, reducing patch sizes by up to 80% and significantly accelerating download and installation times. This architecture improves both player experience and operational efficiency.\nBlog 4 - Securing Amazon EVS with AWS Network Firewall This blog demonstrates how to secure Amazon Elastic VMware Service (EVS) environments using AWS Network Firewall. The architecture applies a centralized inspection model with AWS Transit Gateway, enabling inspection of both north-south and east-west traffic. The solution provides centralized firewall management, logging, and monitoring, and is well suited for hybrid cloud environments.\nBlog 5 - Building an AI Gateway to Amazon Bedrock with Amazon API Gateway This article presents an AI Gateway architecture that places Amazon API Gateway in front of Amazon Bedrock. The solution allows organizations to enforce access control, quotas, rate limits, and cost governance when using foundation models. It supports multi-tenant environments and enables enterprise-scale generative AI deployments with strong security and observability.\nBlog 6 - Priority-Based Message Processing with Amazon MQ and AWS App Runner This blog explains how to build a priority-based message processing system using Amazon MQ, AWS App Runner, and Amazon DynamoDB. The architecture ensures that critical messages are processed ahead of lower-priority workloads while maintaining reliability and scalability. It supports real-time status updates, retry mechanisms, and dead-letter queues, making it suitable for enterprise-grade, event-driven applications.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section list and describe the events you participated in during your internship or work. Provide details per event such as:\nEvent name Date/time Location Your role (attendee, organizer, speaker, etc.) Short description and main activities Outcomes or lessons learned Event 1 Event name: GenAI-powered App-DB Modernization workshop\nTime: 09:00 13/08/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu St., Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event name: GenAI-powered App-DB Modernization workshop\nTime: 09:00 13/08/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu St., Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from 8/9/2025 to 12/9/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment. I participated in First Cloud Journey, through which I improved my skills in Communication, financial management, more knowledge about AWS and Cloud: translation.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What were you most satisfied with during your internship? During my internship at AWS, I was most satisfied with the professional and friendly working environment. My mentors were always willing to support me and helped me gain a lot of practical knowledge about cloud computing, especially AWS services. In addition, I felt very fortunate to be exposed to structured working processes and modern technologies.\nWhat do you think the company should improve for future interns? In my opinion, the company could improve by creating more dedicated guidance materials for interns, as well as organizing more in-depth hands-on training sessions to help new interns quickly become familiar with the system and their tasks.\nIf you were to recommend this place to your friends, would you encourage them to intern here? Why? I would definitely recommend my friends to intern at AWS because it is an excellent environment for learning and professional development. The company has a highly professional culture, offers opportunities to work with new technologies, and helps students build a strong foundation for their future careers.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? I hope the company can organize more workshops where senior engineers share their real-world experience, as well as create more bonding activities among interns to enhance communication and mutual learning.\nWould you like to continue this program in the future? Yes, I would very much like to continue participating in this program in the future if given the opportunity, as I feel this is an ideal environment for personal growth and improving my professional skills.\nOther feedback (free sharing): I would like to sincerely thank AWS and the mentors who have always supported and guided me throughout my internship. This has been a very valuable and meaningful period of my life.\n"},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.10-week10/","title":"Week 10","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: This is a reference template only ‚Äî do not copy verbatim for official submission.\nObjectives of This Week: Understand how to build serverless web applications and integrate APIs. Deploy Node.js applications using Elastic Beanstalk and automate CI/CD with AWS CDK. Learn WordPress architecture on AWS and run WordPress on Amazon EC2. Compare Serverless Web Apps, Elastic Beanstalk, and EC2-hosted WordPress. Tasks to Perform This Week: Day Tasks Start Date End Date Reference 2 - Overview of Serverless Web App Workshop - Build Serverless APIs (Lambda + API Gateway) dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 3 - Build Serverless Chat Application - WebSocket API + DynamoDB Streams dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 4 - Elastic Beanstalk Workshop - Deploy Node.js application on Elastic Beanstalk dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 5 - CI/CD with Elastic Beanstalk + CDK Pipelines - Automated deployment from source ‚Üí production dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 6 - WordPress Architecture on AWS - WordPress Well-Architected design dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 7 - Run WordPress on EC2: + EC2 + RDS + EFS/FSx + ALB dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ Achievements: üü£ 1. Serverless Web App Workshop üîß Building Serverless APIs Built APIs using Amazon API Gateway and AWS Lambda. Implemented routing, stages, integrations, and security (CORS, Cognito authentication). Stored data in DynamoDB and designed tables based on access patterns. üí¨ Serverless Chat Application Used API Gateway WebSocket APIs to build a real-time chat system. Used DynamoDB Streams to push real-time messages to connected clients. Lambda functions handled events in a fully event-driven architecture. The application automatically scaled without server management. üü¢ 2. Elastic Beanstalk Workshop üöÄ Deploying Node.js Applications Uploaded source code to Elastic Beanstalk. Elastic Beanstalk automatically provisioned EC2, Auto Scaling Groups, Load Balancers, and Security Groups. Learned deployment policies: Rolling, Rolling with Additional Batch, Immutable, etc. üîÑ CI/CD with Elastic Beanstalk \u0026amp; CDK Pipelines Built automated pipelines for build and deployment. Used AWS CDK to define Elastic Beanstalk environments and pipeline infrastructure. Reduced deployment errors and delivery time. üîµ 3. WordPress on AWS üèó WordPress Architecture on AWS Learned the standard AWS architecture: EC2 for the PHP application servers Amazon RDS (MySQL) for the database EFS/FSx for shared wp-content storage ALB for load balancing CloudFront + S3 for optimized media delivery Understood how to scale WordPress across multiple instances. üíª Running WordPress on Amazon EC2 Launched EC2 and installed LAMP/LEMP stack. Connected WordPress to Amazon RDS MySQL. Mounted shared storage using EFS/FSx. Configured basic security and backups. Tuned performance and caching. Weekly Summary: Completed hands-on experience with serverless APIs, real-time WebSocket chat, DynamoDB, and event-driven architectures. Became proficient in deploying Node.js applications with Elastic Beanstalk and CI/CD using AWS CDK. Gained solid understanding of optimized WordPress architecture on AWS and practical deployment on EC2. Ready to combine Serverless, Elastic Beanstalk, and EC2 models for real-world projects. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.11-week11/","title":"Week 11","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: This is a reference template ‚Äî do not copy verbatim for official submission.\nObjectives of This Week: Understand the entire AWS Container Services ecosystem. Practice deploying containers using Amazon Lightsail, ECS Fargate, and Amazon EKS. Apply Infrastructure as Code (IaC) for ECS and EKS using AWS CDK and EKS Blueprints. Build CI/CD pipelines for containerized applications. Learn about ROSA ‚Äì Red Hat OpenShift Service on AWS. Tasks to Perform This Week: Day Tasks Start Date End Date Reference 2 - Overview of Container Services on AWS - Lightsail Containers Architecture dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 3 - Getting Started with Amazon Lightsail Containers dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 4 - Amazon ECS Workshop: + Containerization with ECS \u0026amp; Fargate + ECS Cluster, Task, Service dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 5 - IaC for ECS with AWS CDK - CI/CD Pipeline for ECS Applications dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 6 - Amazon EKS Workshop: Getting Started with EKS - EKS Node Groups, VPC, RBAC dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 7 - IaC with EKS Blueprints for CDK - CI/CD for EKS Applications dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 8 - Deploying First Application to Amazon EKS - Terraform with EKS (Coming Soon) dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 9 - Red Hat OpenShift Service on AWS (ROSA) dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ Achievements: üü£ 1. Amazon Lightsail Containers Understood how Lightsail simplifies container deployment with minimal configuration. Pushed Docker images to Lightsail Container Service. Scaled containers and quickly created staging/production environments. Used the built-in Lightsail Load Balancer. üü¢ 2. Amazon ECS Workshop (ECS \u0026amp; Fargate) Containerization with ECS \u0026amp; Fargate Created ECS Clusters (Fargate mode without managing servers). Built Task Definitions: container image, CPU, memory, and port mappings. Created ECS Services with Auto Scaling. Deployed fully containerized applications. Infrastructure as Code with AWS CDK Used CDK to create ECS Clusters, Task Definitions, Services, and Application Load Balancers. IaC enabled reusable and automated infrastructure deployment. CI/CD for ECS Applications Implemented CodePipeline + CodeBuild to build images ‚Üí push to ECR ‚Üí deploy to ECS. Fully automated the CI/CD lifecycle. üîµ 3. Amazon EKS Workshop (Kubernetes on AWS) Getting Started with EKS Learned about the control plane, worker nodes, node groups, and RBAC. Deployed an EKS cluster and configured kubectl. IaC with EKS Blueprints (CDK) Created EKS clusters using CDK Blueprints. Automatically installed add-ons: VPC CNI, CoreDNS, kube-proxy, ALB Controller, etc. Managed clusters using enterprise-grade best practices. CI/CD for EKS Applications Built Docker images and pushed to ECR ‚Üí automatically deployed Kubernetes manifests (YAML). Used Blue/Green and Rolling deployments with Kubernetes. Deploying First App to EKS Created Kubernetes Deployments and Services (ClusterIP/LoadBalancer). Exposed the application using ALB Ingress Controller. Terraform with EKS (Coming Soon) Note: Not implemented yet, planned for future learning. üî¥ 4. Red Hat OpenShift on AWS (ROSA) Understood ROSA as a fully managed OpenShift service on AWS. Integrated enterprise Kubernetes with strong security and compliance. Suitable for hybrid cloud and high-compliance workloads. Supported automated deployment, scaling, and management via OpenShift Console. Weekly Summary: Completed knowledge from simple container platforms (Lightsail) to enterprise orchestration (EKS). Became proficient in ECS Fargate, CDK-based IaC, and CI/CD for container workloads. Gained hands-on experience with Kubernetes architecture on Amazon EKS. Explored ROSA as an enterprise-grade container solution on AWS. Ready to design and operate modern container infrastructure for real-world projects. "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.12-week12/","title":"Week 12","tags":[],"description":"","content":"\r‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Review all core AWS services learned during the internship. Design and implement a final project integrating multiple AWS services. Prepare final report and presentation. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review all core services: EC2, S3, RDS, DynamoDB, IAM, VPC, Lambda, CloudWatch, CloudFront, API Gateway - Define requirements and architecture for the final project 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Start project deployment: + Design VPC, subnet, security groups + Configure S3, CloudFront, RDS/DynamoDB (depending on project) 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Continue project implementation: + Build backend using Lambda / API Gateway or EC2 (depending on architecture) + Connect database and process data + Integrate CloudWatch for monitoring 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Complete the project: + Add Cognito authentication if needed + Finalize CI/CD pipeline (CodePipeline/CodeBuild) + End-to-end system testing 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Write final report - Prepare presentation (architecture, service selection rationale, cost, security) - Summarize the entire learning journey and self-evaluate capabilities 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Reviewed Core AWS Services:\nEC2, S3, RDS, DynamoDB IAM, VPC, Security Groups Lambda, API Gateway CloudWatch, CloudFront Cognito, CodePipeline Planned Final Project:\nDefined project requirements\nDesigned system architecture Selected appropriate AWS services Created architecture diagram Planned implementation steps Implemented Project Infrastructure:\nDesigned VPC with public and private subnets Configured security groups and network ACLs Set up S3 buckets for storage Configured CloudFront for content delivery Created RDS database instance Built Application Backend:\nImplemented Lambda functions for business logic Created API Gateway REST endpoints Connected Lambda to database Configured CloudWatch logs and monitoring Set up error handling and retry logic Completed Project Features:\nIntegrated Cognito for user authentication Set up CodePipeline for CI/CD Performed end-to-end testing Fixed bugs and optimized performance Verified all features working correctly Prepared Final Deliverables:\nWrote comprehensive project report Created presentation slides covering: Architecture overview Service selection rationale Security implementation Cost estimation Summarized 12-week learning journey Self-evaluated technical capabilities gained "},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thinhpxse.github.io/fcj-workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]