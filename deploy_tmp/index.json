[
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Phan Xuan Thinh\nPhone Number: 0945645753\nEmail: thinhpxse184527@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Creating Gateway Endpoint for S3",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you\u0026rsquo;ll configure a Gateway VPC Endpoint to enable private connectivity between your VPC and Amazon S3. This eliminates the need for internet gateway, NAT devices, or VPN connections, while keeping traffic within the AWS network.\nWhat is a Gateway Endpoint? Gateway endpoints are route-based endpoints that appear as targets in your VPC route tables. When you create a gateway endpoint for S3 or DynamoDB, AWS automatically updates the specified route tables with routes directing traffic to the endpoint. This approach:\nIncurs no additional data transfer charges Doesn\u0026rsquo;t require elastic network interfaces (ENIs) Scales automatically to handle your workload Benefits:\nCost Efficiency: No NAT Gateway charges for S3 access Security: Traffic never leaves the AWS network Performance: Lower latency compared to internet-based access Simplified Architecture: No need to manage NAT or internet gateway for S3 access Step 1: Navigate to VPC Endpoints Console Access the Amazon VPC Console in the us-east-1 region From the left navigation menu, locate and click on Endpoints Click the Create endpoint button in the upper right Expected Observation: You should see 6 pre-existing VPC endpoints in the list. These are Interface Endpoints for AWS Systems Manager (SSM) services that were automatically provisioned by the CloudFormation template during setup. These SSM endpoints enable secure, private connectivity to Systems Manager without requiring public internet access, allowing you to manage EC2 instances through Session Manager.\nStep 2: Configure Endpoint Basic Settings In the Create endpoint configuration page:\nName tag: Enter a descriptive name: s3-gateway-endpoint\nThis helps identify the endpoint\u0026rsquo;s purpose in a multi-endpoint environment Service category: Select AWS services\nThis option allows you to choose from AWS-managed service endpoints Step 3: Select the S3 Gateway Service In the Services section, use the search functionality:\nType s3 in the filter box The list will display S3-related endpoint options Important: Select the service entry where:\nService Name contains com.amazonaws.us-east-1.s3 Type column shows Gateway Do NOT select the Interface type endpoint for S3.\nUnderstanding the Difference:\nGateway endpoint (what we\u0026rsquo;re creating): Uses route table entries, free data transfer Interface endpoint: Uses ENIs, incurs PrivateLink charges Step 4: Associate VPC and Route Tables VPC Selection:\nFrom the VPC dropdown menu, select VPC Cloud This is the VPC where your cloud resources reside Route Tables Configuration:\nIn the Route tables section, you\u0026rsquo;ll see all route tables associated with the selected VPC Select the route table that shows association with 2 subnets Important: This is NOT the main route table of the VPC. It\u0026rsquo;s a custom route table created by CloudFormation specifically for the private subnets where your EC2 instances are running. The gateway endpoint will automatically add a route to this table directing S3 traffic (pl-63a5400a prefix list) to the endpoint.\nWhat Happens Behind the Scenes: When you select the route table, AWS will automatically add an entry like:\nDestination: pl-63a5400a (S3 prefix list)\rTarget: vpce-xxxxx (your gateway endpoint ID) Step 5: Configure Endpoint Policy Policy Type: Leave the default Full access option selected\nThis grants unrestricted access to all S3 operations and buckets through this endpoint.\nNote: In a later section of this workshop, you will learn how to implement restrictive VPC endpoint policies to limit access to specific S3 buckets or operations. This demonstrates the principle of least privilege and shows how endpoint policies provide an additional security layer beyond IAM and bucket policies.\nPolicy Options Explained:\nFull access: Allows all S3 actions on all resources Custom: Lets you define specific allowed/denied actions and resources using JSON policy Step 6: Review and Create Endpoint Tags (Optional): Skip adding tags for this workshop\nIn production, consider adding tags like Environment: Workshop, Owner: YourName Click Create endpoint to provision the gateway endpoint\nConfirmation: You should see a success message indicating the endpoint was created\nClick the X or Close button to return to the endpoints list\nVerification:\nThe new endpoint appears in your endpoints list with status available Check the associated route table to see the new S3 prefix list route Understanding What Was Created Your gateway endpoint is now active and has automatically:\nUpdated the route table with an entry pointing S3 traffic to the endpoint Created a managed prefix list containing all S3 IP ranges for us-east-1 Enabled private S3 access for resources in the associated subnets Next Steps: In the following section, you\u0026rsquo;ll test this endpoint by accessing S3 from an EC2 instance in your VPC and verify that traffic flows through the private endpoint instead of the internet.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Environment Preparation for Hybrid Connectivity",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you\u0026rsquo;ll prepare the environment to simulate hybrid cloud connectivity between an on-premises datacenter and AWS. This setup demonstrates how organizations can securely access AWS services like S3 from their corporate networks through private connections.\nWhat You\u0026rsquo;ll Configure:\nDeploy additional DNS infrastructure using CloudFormation Configure VPN routing to enable on-premises to cloud connectivity Architecture Context: This configuration simulates a real-world hybrid architecture where:\nAn on-premises datacenter connects to AWS via Site-to-Site VPN DNS queries from on-premises are resolved through Route 53 S3 access from on-premises flows through Interface VPC Endpoints All traffic remains private without internet exposure Part 1: Deploy DNS Infrastructure with CloudFormation To enable proper DNS resolution in the hybrid environment, you\u0026rsquo;ll deploy a CloudFormation stack that creates Route 53 resolver endpoints and private hosted zones.\nWhat This Stack Creates:\nThe CloudFormation template provisions three critical DNS components:\nRoute 53 Private Hosted Zone\nHosts DNS records (Alias records) for the S3 Interface Endpoint Allows on-premises systems to resolve S3 endpoint names to private IP addresses Scoped to your VPC for private resolution Route 53 Inbound Resolver Endpoint\nDeployed in \u0026ldquo;VPC Cloud\u0026rdquo; Receives DNS queries from on-premises environments Forwards queries to the Private Hosted Zone for resolution Creates elastic network interfaces (ENIs) in your VPC subnets Route 53 Outbound Resolver Endpoint\nDeployed in \u0026ldquo;VPC On-prem\u0026rdquo; Forwards DNS queries for S3 domains to \u0026ldquo;VPC Cloud\u0026rdquo; Enables conditional DNS forwarding based on domain names Routes queries through the VPN tunnel to the Inbound Resolver DNS Flow Diagram:\nDeployment Steps:\nLaunch CloudFormation Stack:\nClick this link to open the CloudFormation console with pre-configured template: Deploy DNS Infrastructure Stack\nThe template URL is pre-populated and the stack name is set to PLOnpremSetup.\nReview and Accept Defaults:\nAll parameters are pre-configured with optimal values The template will automatically detect your VPC IDs and subnet configurations Scroll to the bottom of the page Acknowledge and Create:\nCheck the acknowledgment box: \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources.\u0026rdquo; Click Create stack Monitor Deployment (Optional):\nStack creation typically takes 3-5 minutes You can monitor progress in the Events tab You don\u0026rsquo;t need to wait - proceed to the next section while this deploys Background Processing: The CloudFormation stack creates resolver endpoints which can take a few minutes. The next configuration step (routing table update) can be completed in parallel while CloudFormation runs.\nWhat Gets Created:\n2 Resolver Endpoint ENIs (Inbound in VPC Cloud) 2 Resolver Endpoint ENIs (Outbound in VPC On-prem) 1 Private Hosted Zone for S3 endpoint DNS resolution Resolver rules for conditional DNS forwarding Part 2: Configure VPN Routing for On-Premises Connectivity Now you\u0026rsquo;ll configure the routing table in \u0026ldquo;VPC On-prem\u0026rdquo; to direct traffic destined for the cloud environment through the VPN tunnel.\nUnderstanding the VPN Setup:\nThis workshop uses a software-based VPN solution running on an EC2 instance to simulate the connection between your simulated on-premises environment and AWS:\nVPN Software: strongSwan (open-source IPsec-based VPN) VPN Gateway Instance: An EC2 instance acting as the customer gateway Connection Type: Site-to-Site VPN tunnel Purpose: Simulates a real datacenter\u0026rsquo;s VPN appliance Production Note: In real-world deployments, you would use:\nPhysical or virtual VPN appliances in your datacenter AWS Site-to-Site VPN with redundant connections AWS Transit Gateway for centralized connectivity Multiple VPN tunnels for high availability Configuration Steps:\nStep 1: Identify the VPN Gateway Instance\nOpen the Amazon EC2 Console\nLocate and select the EC2 instance with the name infra-vpngw-test\nThis instance is running strongSwan VPN software It\u0026rsquo;s configured to terminate the VPN tunnel from \u0026ldquo;VPC Cloud\u0026rdquo; In the Details tab (bottom pane), locate the Instance ID\nFormat: i-0123456789abcdef0 Copy this Instance ID to your clipboard or text editor You\u0026rsquo;ll need this ID in the next steps Instance Role: This EC2 instance has IP forwarding enabled and is configured with IPsec tunnels. It acts as the gateway between the simulated on-premises network and AWS Transit Gateway.\nStep 2: Navigate to VPC Route Tables\nUse the search box at the top of the AWS Console Type VPC and press Enter From the VPC Dashboard left menu, click Route Tables Step 3: Update On-Premises Route Table\nLocate the correct route table:\nFilter or search for the route table named RT Private On-prem This is the route table associated with the private subnets in \u0026ldquo;VPC On-prem\u0026rdquo; Click on the route table to select it Access route editing:\nClick on the Routes tab in the bottom details pane Click Edit routes button Add new route entry:\nClick Add route button Configure the new route:\nDestination: Enter your Cloud VPC CIDR block\nFormat: 10.0.0.0/16 (or whatever CIDR your Cloud VPC uses) This tells the route table where cloud resources are located Target: Select Instance, then choose the infra-vpngw-test instance\nPaste the Instance ID you copied earlier Alternatively, start typing the instance name and select from the dropdown Save the configuration: Click Save changes The route table is immediately updated Understanding What You Configured:\nThis route entry tells instances in \u0026ldquo;VPC On-prem\u0026rdquo; that:\nTraffic destined for Cloud VPC CIDR ‚Üí Send to VPN Gateway Instance Traffic Flow:\nOn-Prem EC2 Instance ‚Üí Route Table Lookup ‚Üí VPN Gateway EC2 ‚Üí IPsec Tunnel ‚Üí Transit Gateway ‚Üí VPC Cloud Routing Configured! Now any traffic from \u0026ldquo;VPC On-prem\u0026rdquo; destined for resources in \u0026ldquo;VPC Cloud\u0026rdquo; will be routed through the VPN tunnel. This simulates how your corporate network would route traffic to AWS over a Site-to-Site VPN connection.\nVerification Steps Verify CloudFormation Stack:\nReturn to CloudFormation Console Check that PLOnpremSetup stack shows CREATE_COMPLETE status Click on the stack and view the Resources tab to see created components Verify Route Table:\nGo back to the RT Private On-prem route table Confirm the new route appears with: Destination: Cloud VPC CIDR Target: VPN Gateway instance ID Status: Active What\u0026rsquo;s Next: With DNS infrastructure and routing in place, you\u0026rsquo;re ready to:\nCreate an Interface VPC Endpoint for S3 Test S3 access from the simulated on-premises environment Verify DNS resolution through Route 53 Resolver Confirm all traffic flows through private connections Architecture Summary You\u0026rsquo;ve now established the foundational hybrid connectivity:\nDNS Resolution Path:\nOn-Prem Application ‚Üí Outbound Resolver Endpoint ‚Üí VPN Tunnel ‚Üí Inbound Resolver Endpoint ‚Üí Private Hosted Zone ‚Üí S3 Endpoint IP Data Path:\nOn-Prem Application ‚Üí VPN Gateway ‚Üí IPsec Tunnel ‚Üí Transit Gateway ‚Üí VPC Cloud ‚Üí Interface Endpoint ‚Üí S3 This architecture demonstrates AWS best practices for:\nPrivate connectivity to AWS services from on-premises DNS resolution in hybrid environments Secure data transfer without internet exposure Scalable VPN connectivity through Transit Gateway "
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nUnderstanding VPC Endpoints VPC Endpoints enable private connectivity between your Virtual Private Cloud (VPC) and AWS services without requiring traffic to traverse the public internet. These endpoints are:\nHighly Available: Built with redundancy across multiple Availability Zones Scalable: Automatically scale to handle your workload demands Secure: Keep traffic within the AWS network, reducing exposure to internet-based threats There are two primary types of VPC endpoints:\nGateway Endpoints: Used for connecting to Amazon S3 and DynamoDB. These are specified as route table targets for traffic destined to supported AWS services. Interface Endpoints (AWS PrivateLink): Enable private connectivity to services powered by AWS PrivateLink, including many AWS services, your own services, and SaaS applications. These create elastic network interfaces (ENIs) in your subnets. Lab Architecture Overview This hands-on workshop demonstrates secure access to Amazon S3 from multiple network environments using VPC endpoints. The lab architecture consists of two distinct Virtual Private Clouds:\nCloud VPC Environment:\nHosts cloud-native AWS resources including EC2 instances for testing Contains a Gateway VPC Endpoint providing direct, private access to Amazon S3 Represents your production AWS cloud infrastructure Demonstrates how cloud workloads can securely access S3 without internet exposure On-Premises Simulation VPC:\nEmulates a traditional on-premises data center or branch office environment Includes an EC2 instance configured with VPN software (OpenSwan/strongSwan) to establish secure connectivity Connected to AWS through a Site-to-Site VPN via AWS Transit Gateway Features an Interface VPC Endpoint enabling on-premises resources to access S3 privately over the VPN connection Simulates hybrid cloud scenarios where on-premises applications need secure S3 access Network Connectivity: The two VPCs are interconnected through AWS Transit Gateway, which acts as a cloud router to enable secure communication between your cloud and simulated on-premises environments. The Site-to-Site VPN connection ensures encrypted traffic flow between the environments.\nImportant Notes:\nThis lab uses a single VPN tunnel for cost efficiency and simplicity For production deployments, AWS strongly recommends implementing redundant VPN connections across multiple devices and Availability Zones for high availability The architecture follows AWS Well-Architected Framework principles for security and reliability This workshop will guide you through configuring both Gateway and Interface VPC Endpoints, testing connectivity from both environments, and understanding best practices for securing S3 access in hybrid cloud architectures.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúCloud Day Vietnam 2025 : AI Edition‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.6-blog6/",
	"title": "Build Priority-Based Message Processing with Amazon MQ and AWS App Runner",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference only. Please do not copy it verbatim into official reports, including this notice.\nBuild Priority-Based Message Processing with Amazon MQ and AWS App Runner Many enterprise systems must process certain tasks immediately (such as urgent orders, system alerts, or mission-critical workflow events), while less important tasks can be processed later without affecting priority workloads. This blog explains how to build a priority-based message processing system using Amazon MQ, AWS App Runner, and Amazon DynamoDB ‚Äî ensuring high-priority messages are processed first, while low-priority events are still handled reliably in the background.\n(aws.amazon.com)\nSolution Overview The system uses several AWS fully managed services to create a scalable, cost-efficient, and reliable priority-based processing architecture:\nAmazon MQ as the message broker supporting JMS priority queues, ensuring high-priority messages are delivered and processed first. AWS App Runner as the fully managed service that runs the message-processing application with automatic scaling based on load. Amazon DynamoDB as a storage layer for message metadata and processing status, including real-time updates via DynamoDB Streams.\n(aws.amazon.com) This solution enables:\nMultiple priority levels: High, Standard, Low High-priority messages immediately move to the queue and bypass delays Standard and low-priority messages can be delayed or queued behind higher-priority traffic Real-time UI updates for message status tracking Two-layer retry and DLQ handling for durability and fault tolerance\n(aws.amazon.com) Priority Flow \u0026amp; Key Processing Mechanics Message intake \u0026amp; priority classification\nThe backend assigns JMS priorities (e.g., High = 9, Standard = 4, Low = 0). For Standard/Low messages, the application can apply a delay before enqueueing.\n(aws.amazon.com) Amazon MQ queueing\nMessages are stored in Amazon MQ queues that support priority ordering. The broker ensures high-priority messages always get delivered first. AWS App Runner processing\nApp Runner hosts a containerized message processor that automatically scales with throughput. Each message is processed and its status is stored in DynamoDB. Real-time monitoring via WebSockets\nDynamoDB Streams push status changes to the UI through WebSockets, enabling users to track message progress live.\n(aws.amazon.com) Retries \u0026amp; fault handling\nFailed messages are retried based on configured policies. Messages that continue to fail after retry limits can be moved to a dead-letter queue (DLQ).\n(aws.amazon.com) Benefits \u0026amp; Use Cases üîπ Key Benefits Guarantees that urgent, critical tasks are processed ahead of lower-priority work Fully managed architecture with minimal operational overhead Real-time updates improve user transparency and system observability Automatic scaling using App Runner ‚Äî no servers or capacity planning required Reliable event-driven design with retries and DLQ for durability\n(aws.amazon.com) üìå Ideal Use Cases Systems with mixed workloads: urgent vs. background tasks Order processing systems that include ‚Äúrush‚Äù or ‚ÄúVIP‚Äù orders Real-time monitoring dashboards that show progress of submitted requests Workflows that require predictable handling of priority traffic Applications where reliability and processing transparency matter Recommendations \u0026amp; Design Notes Configure Amazon MQ queues to fully support priority ordering; JMS priority must be enabled at the broker level. For production workloads, prefer native delay mechanisms of Amazon MQ (such as scheduled delivery or message TTL) rather than doing delay logic inside the application. Use IAM least privilege for App Runner, giving it only the permissions required for MQ and DynamoDB. Place Amazon MQ in private subnets for better security; avoid public accessibility if not needed.\n(aws.amazon.com) Conclusion By combining Amazon MQ + AWS App Runner + Amazon DynamoDB, you can build a scalable, fault-tolerant, and priority-aware message processing system.\nHigh-priority messages are processed quickly, lower-priority messages are handled efficiently in the background, and users get real-time visibility into the processing lifecycle.\nThis design is ideal for systems requiring:\nmulti-level message prioritization real-time message status updates high availability \u0026amp; fault tolerance minimum operational burden It offers strong reliability, clear priority control, and operational simplicity ‚Äî all powered by fully managed AWS services.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.5-blog5/",
	"title": "Building an AI Gateway to Amazon Bedrock with Amazon API Gateway",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference only. Please do not copy verbatim into official reports, including this notice.\nBuilding an AI Gateway to Amazon Bedrock with Amazon API Gateway When building generative-AI applications at enterprise scale, organizations often need governance over foundation model usage ‚Äî such as authorization, quota management, tenant isolation, and cost control. This blog presents a reference pattern (originally implemented by a company) that uses Amazon API Gateway as the front layer to manage access to Amazon Bedrock services, providing enterprise-grade controls, while remaining transparent to client applications. :contentReference[oaicite:2]{index=2}\nAI Gateway Architecture Overview The reference architecture offers granular control over access to foundation models using fully managed AWS services; it\u0026rsquo;s transparent to clients and integrates seamlessly into existing enterprise environments. :contentReference[oaicite:3]{index=3}\nCore components:\nCustom domain routing (optional): using a DNS service (e.g. Amazon Route 53), so clients access the gateway via a company-specific domain rather than the default API Gateway domain. :contentReference[oaicite:5]{index=5} Amazon API Gateway: serves as the entry point for all calls to Bedrock ‚Äî handles authorization, request throttling / quota management, lifecycle controls, and integrates with security tools (e.g. WAF). :contentReference[oaicite:6]{index=6} Authorizer (e.g. AWS Lambda Authorizer): validates authentication tokens (like JWT) or integrates with existing identity systems (or other API-Gateway auth mechanisms) to enforce access control per tenant/user. :contentReference[oaicite:7]{index=7} Integration Lambda (request forwarder): receives requests from API Gateway, signs them with AWS credentials (AWS Signature v4), forwards them to the correct Amazon Bedrock endpoint ‚Äî preserving the original request details (parameters, action, etc.). This way, the gateway supports current and future Bedrock APIs without requiring updates to gateway code. :contentReference[oaicite:8]{index=8} Amazon Bedrock: provides foundation models and generative-AI capabilities behind the gateway. :contentReference[oaicite:9]{index=9} This pattern ensures clients can use standard SDKs (e.g. boto3) to call Bedrock as usual, but every request first passes through the gateway for governance. :contentReference[oaicite:10]{index=10}\nDeploying \u0026amp; Testing the Gateway The blog provides deployment instructions using infrastructure-as-code (e.g. via AWS CloudFormation). Initial deployment can disable authorization to simplify testing (e.g. private/regional endpoint, no JWT validation). :contentReference[oaicite:12]{index=12} After stack creation, you get outputs like GatewayUrl, VpcId, ApiId ‚Äî which you can use to test the gateway from inside the VPC (for example using a CloudShell environment). :contentReference[oaicite:13]{index=13} Example code is provided (e.g. Python + boto3) to create a client that works through the gateway. This client sets up custom headers (including service endpoint prefix) but signs request at the gateway side, thus preserving compatibility with Bedrock SDK calls. :contentReference[oaicite:14]{index=14} The blog shows how to call streaming APIs (e.g. for LLM inference) via gateway ‚Äî since Amazon API Gateway now supports response streaming, responses can be delivered to clients in real-time as the model generates them. :contentReference[oaicite:15]{index=15} Optionally, after basic validation, you can enable authorization (e.g. JWT token validation) by updating the CloudFormation stack parameter. Then re-deploy API Gateway to activate changes. :contentReference[oaicite:16]{index=16}\nAdditional Gateway Capabilities \u0026amp; Best Practices Using API Gateway plus this integration pattern gives you many enterprise-level controls:\nRate limiting \u0026amp; throttling / usage quotas: to control usage per tenant/user ‚Äî important for multi-tenant SaaS or shared internal platforms. :contentReference[oaicite:17]{index=17} Lifecycle management \u0026amp; canary releases: you can manage API versioning, gradually roll out new versions, use stage variables, and do safe deployments. :contentReference[oaicite:18]{index=18} Security hardening: integrate with AWS WAF to filter common web threats. :contentReference[oaicite:20]{index=20} Caching or prompt/response caching: for stable or repeated requests, to reduce cost and latency. :contentReference[oaicite:21]{index=21} Content filtering / custom validation: in the Lambda integration layer, you can implement additional checks ‚Äî e.g. filter sensitive content, sanitize inputs/outputs ‚Äî as a second line of defense beyond Bedrock‚Äôs built-in safety mechanisms. :contentReference[oaicite:22]{index=22} Benefits \u0026amp; Use-Cases This AI gateway pattern is particularly useful when you need:\nMulti-tenant access to LLM services ‚Äî each tenant gets per-tenant quota, isolation, usage tracking Governance and control over generative AI usage (authorization, cost control, rate limiting) Seamless integration with existing enterprise identity/auth systems (e.g. via JWT, Cognito, custom authorizers) Transparent SDK compatibility ‚Äî clients can still use Bedrock‚Äôs SDKs (e.g. boto3) as-is, without custom code changes Safe rollout and version control of AI APIs (canary deployments, staged rollout) In short ‚Äî you get enterprise-grade controls + flexibility + scalability + ease of adoption.\nConclusion The ‚ÄúAI Gateway to Amazon Bedrock with Amazon API Gateway‚Äù pattern offers a robust, scalable, and secure architecture for deploying generative-AI services in production. By putting a fully managed gateway in front of Bedrock, organizations gain fine-grained control over who can access what, how much, and when ‚Äî while preserving compatibility with existing tools and SDKs. It‚Äôs a powerful base for building multi-tenant AI services, internal AI-powered platforms, or scalable AI backends for customers.\nIf you want ‚Äî I can also generate a diagram (ASCII or markdown) of the architecture for easier inclusion in your documentation. ::contentReference[oaicite:23]{index=23}\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Electronic Arts t·ªëi ∆∞u h√≥a quy tr√¨nh c·∫≠p nh·∫≠t game v·ªõi AWS",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è L∆∞u √Ω: Th√¥ng tin d∆∞·ªõi ƒë√¢y ch·ªâ nh·∫±m m·ª•c ƒë√≠ch tham kh·∫£o. Vui l√≤ng kh√¥ng sao ch√©p nguy√™n vƒÉn cho b√°o c√°o c·ªßa b·∫°n, bao g·ªìm c·∫£ ph·∫ßn n√†y.\nElectronic Arts t·ªëi ∆∞u h√≥a quy tr√¨nh c·∫≠p nh·∫≠t game v·ªõi AWS Quy tr√¨nh c·∫≠p nh·∫≠t (patching) game lu√¥n l√† th√°ch th·ª©c l·ªõn: dung l∆∞·ª£ng game c√≥ th·ªÉ l√™n ƒë·∫øn h√†ng ch·ª•c ho·∫∑c h√†ng trƒÉm GB, vi·ªác so s√°nh d·ªØ li·ªáu tr√™n m√°y ng∆∞·ªùi ch∆°i th∆∞·ªùng ch·∫≠m v√† d·∫´n ƒë·∫øn k√≠ch th∆∞·ªõc patch l·ªõn, th·ªùi gian c√†i ƒë·∫∑t d√†i v√† tr·∫£i nghi·ªám k√©m. B√†i blog gi·ªõi thi·ªáu c√°ch Electronic Arts (EA) s·ª≠ d·ª•ng AWS ƒë·ªÉ hi·ªán ƒë·∫°i h√≥a quy tr√¨nh patching, gi·∫£m k√≠ch th∆∞·ªõc b·∫£n c·∫≠p nh·∫≠t, tƒÉng t·ªëc t·∫£i xu·ªëng v√† c·∫£i thi·ªán tr·∫£i nghi·ªám ng∆∞·ªùi ch∆°i.\nTh√°ch th·ª©c ban ƒë·∫ßu: Patching ch·∫≠m v√† k√©m hi·ªáu qu·∫£ Patching truy·ªÅn th·ªëng ch·ªâ so s√°nh file theo t√™n, khi·∫øn d·ªØ li·ªáu b·ªã di chuy·ªÉn gi·ªØa c√°c file b·ªã coi nh∆∞ ‚Äúho√†n to√†n m·ªõi‚Äù ‚Üí ng∆∞·ªùi ch∆°i ph·∫£i t·∫£i l·∫°i nhi·ªÅu GB d·ªØ li·ªáu. K√≠ch th∆∞·ªõc kh·ªëi d·ªØ li·ªáu (block size) d√πng ƒë·ªÉ so s√°nh tr√™n m√°y kh√°ch r·∫•t l·ªõn (nh∆∞ 64 KB), l√†m gi·∫£m kh·∫£ nƒÉng t√°i s·ª≠ d·ª•ng d·ªØ li·ªáu c≈©, khi·∫øn patch ph√¨nh to. Vi·ªác t√≠nh to√°n patch di·ªÖn ra tr√™n m√°y ng∆∞·ªùi ch∆°i m·∫•t nhi·ªÅu ph√∫t ho·∫∑c l√¢u h∆°n, ƒë·∫∑c bi·ªát v·ªõi c√°c game l·ªõn ‚Äî ·∫£nh h∆∞·ªüng tr·∫£i nghi·ªám. Gi·∫£i ph√°p: Known Version Patching (KVP) ch·∫°y tr√™n AWS Th√°ng 8/2023, EA gi·ªõi thi·ªáu h·ªá th·ªëng Known Version Patching (KVP) ‚Äî quy tr√¨nh t√≠nh to√°n patch tr√™n server (server-side) ch·∫°y ho√†n to√†n tr√™n AWS.\nKVP ho·∫°t ƒë·ªông nh∆∞ sau: M·ªói phi√™n b·∫£n game ƒë∆∞·ª£c t·∫£i v·ªÅ v√† gi·∫£i n√©n tr√™n AWS; h·ªá th·ªëng t√≠nh to√°n hash cho t·ª´ng phi√™n b·∫£n. So s√°nh metadata hash c·ªßa phi√™n b·∫£n m·ªõi v·ªõi t·∫•t c·∫£ phi√™n b·∫£n c≈© ƒë·ªÉ x√°c ƒë·ªãnh ch√≠nh x√°c ph·∫ßn d·ªØ li·ªáu thay ƒë·ªïi. T·∫°o c√°c b·∫£n patch incremental (nh·ªè ch·ªâ ch·ª©a d·ªØ li·ªáu thay ƒë·ªïi). M·ªôt microservice ƒëi·ªÅu ph·ªëi (coordinator) nh·∫≠n s·ª± ki·ªán khi c√≥ phi√™n b·∫£n m·ªõi, sau ƒë√≥ k√≠ch ho·∫°t pipeline patching v√† c√°c job x·ª≠ l√Ω song song. Amazon Elastic Kubernetes Service (EKS) d√πng ƒë·ªÉ ch·∫°y c√°c job song song quy m√¥ l·ªõn. Amazon Elastic File System (EFS) l∆∞u tr·ªØ b·∫£n build ƒë√£ gi·∫£i n√©n, cho ph√©p t√°i s·ª≠ d·ª•ng v√† tr√°nh t·∫£i l·∫°i ho·∫∑c t√≠nh to√°n l·∫°i cho c√°c phi√™n b·∫£n c≈©. Ki·∫øn tr√∫c k·ªπ thu·∫≠t \u0026amp; quy tr√¨nh patch Khi c√≥ version m·ªõi, h·ªá th·ªëng streaming (v√≠ d·ª• Amazon MSK) g·ª≠i th√¥ng b√°o ƒë·ªÉ k√≠ch ho·∫°t pipeline. Service ƒëi·ªÅu ph·ªëi t·∫°o c√°c job EKS ƒë·ªÉ t·∫£i, gi·∫£i n√©n, t√≠nh hash, so s√°nh metadata v√† t·∫°o patch. Patch ƒë∆∞·ª£c upload l√™n CDN ƒë·ªÉ ph√¢n ph·ªëi t·ªõi ng∆∞·ªùi ch∆°i qua ·ª©ng d·ª•ng EA. EFS cache l·∫°i c√°c b·∫£n build ƒë√£ gi·∫£i n√©n ‚Äì l·∫ßn patch sau ch·ªâ so s√°nh ph·∫ßn thay ƒë·ªïi ‚Üí ti·∫øt ki·ªám th·ªùi gian v√† t√†i nguy√™n. EKS d√πng CPU affinity, autoscaling v√† t·ªëi ∆∞u scheduling ƒë·ªÉ tƒÉng t·ªëi ƒëa th√¥ng l∆∞·ª£ng c·ªßa cluster. L·ª£i √≠ch \u0026amp; c·∫£i thi·ªán ƒë·∫°t ƒë∆∞·ª£c Gi·∫£m k√≠ch th∆∞·ªõc patch: Nh·ªù ch·ªâ ch·ª©a d·ªØ li·ªáu thay ƒë·ªïi, patch gi·∫£m ƒë·∫øn ~80% so v·ªõi tr∆∞·ªõc. T·∫£i xu·ªëng v√† c√†i ƒë·∫∑t nhanh h∆°n: Ng∆∞·ªùi ch∆°i t·∫£i v√† c√†i ƒë·∫∑t patch nhanh h∆°n t·ªëi ƒëa 3.6 l·∫ßn. C·∫£i thi·ªán tr·∫£i nghi·ªám ng∆∞·ªùi ch∆°i: √çt ch·ªù ƒë·ª£i h∆°n, gi·∫£m kh√≥ ch·ªãu khi c·∫≠p nh·∫≠t game. T·ªëi ∆∞u chi ph√≠ \u0026amp; t√†i nguy√™n: D·ª±a tr√™n h·∫° t·∫ßng AWS, EA m·ªü r·ªông linh ho·∫°t thay v√¨ ph·ª• thu·ªôc v√†o kh·∫£ nƒÉng m√°y ng∆∞·ªùi ch∆°i. TƒÉng t·ªëc ph√°t tri·ªÉn: T·ª± ƒë·ªông h√≥a patch gi√∫p chu k·ª≥ ph√°t h√†nh v√† ki·ªÉm th·ª≠ nhanh h∆°n nhi·ªÅu. K·∫øt lu·∫≠n B·∫±ng c√°ch chuy·ªÉn qu√° tr√¨nh t√≠nh to√°n patch l√™n AWS v√† √°p d·ª•ng h·ªá th·ªëng KVP (EKS + EFS), EA ƒë√£ t·∫°o ra b∆∞·ªõc nh·∫£y v·ªçt trong hi·ªáu qu·∫£ c·∫≠p nh·∫≠t game: b·∫£n c·∫≠p nh·∫≠t nh·ªè h∆°n, nhanh h∆°n, ti·∫øt ki·ªám chi ph√≠ v√† ·ªïn ƒë·ªãnh h∆°n. Ng∆∞·ªùi ch∆°i nh·∫≠n patch nhanh h∆°n, trong khi EA c√≥ th·ªÉ r√∫t ng·∫Øn th·ªùi gian ph√°t tri·ªÉn v√† gi·∫£m t·∫£i cho h·∫° t·∫ßng n·ªôi b·ªô.\nGi·∫£i ph√°p n√†y ch·ª©ng minh s·ª©c m·∫°nh c·ªßa AWS trong vi·ªác x·ª≠ l√Ω kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu l·ªõn, quy m√¥ to√†n c·∫ßu v√† cung c·∫•p tr·∫£i nghi·ªám v∆∞·ª£t tr·ªôi cho c·∫£ nh√† ph√°t tri·ªÉn l·∫´n game th·ªß.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: ‚ÄúGenAI-powered App-DB Modernization workshop‚Äù Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah ‚Äì Director, Open Source Databases Erica Liu ‚Äì Sr. GTM Specialist, AppMod Fabrianne Effendi ‚Äì Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles ‚Üí Lost revenue/missed opportunities Inefficient operations ‚Üí Reduced productivity, higher costs Non-compliance with security regulations ‚Üí Security breaches, loss of reputation Transitioning to modern application architecture ‚Äì Microservices Migrating to a modular system ‚Äî each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events ‚Üí arrange timeline ‚Üí identify actors ‚Üí define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 ‚Üí ECS ‚Üí Fargate ‚Üí Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing ‚Äî follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the ‚ÄúGenAI-powered App-DB Modernization‚Äù workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Learn About Machine Learning on AWS",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nLet‚Äôs Architect! Learn About Machine Learning on AWS Machine Learning (ML) is becoming essential for organizations wanting to make data-driven decisions, automate processes, and build intelligent applications. ML models are not static ‚Äî they continuously improve as more data flows in, helping organizations adapt to business changes.\nThis blog highlights several AWS resources, architectures, workshops, and customer stories to help teams design, deploy, and scale ML workloads effectively.\nZero to Machine Learning: Jump-Start Your Data-Driven Journey A session from AWS re:Invent 2023 showing how organizations with limited resources (time, budget, expertise) can quickly start ML workloads on AWS.\nKey ideas:\nUse analytics + ML to build end-to-end data pipelines. Apply low-code / no-code tooling to accelerate ML adoption. Leverage AWS services to reduce operational overhead and shorten time-to-value. Introduction to MLOps Engineering on AWS MLOps extends DevOps practices to ML workloads and helps teams automate and manage the entire machine-learning lifecycle.\nCore concepts include:\nVersioning data, models, experiments. Automated training, testing, and deployment pipelines. Monitoring model performance after deployment. Ensuring reliability and scalability of ML systems. This section provides architecture patterns and best practices to adopt MLOps using AWS services.\nGenerative AI Infrastructure at Amazon An inside look at AWS Trainium and AWS Inferentia ‚Äî custom silicon built to optimize ML training and inference at scale.\nHighlights:\nReduce training costs for deep learning \u0026amp; generative AI models. Lower inference latency for production workloads. Improve performance for tasks like LLMs, computer vision, and recommendation systems. These accelerators help customers scale ML workloads efficiently.\nCustomer Stories Pinterest Pinterest shares how they:\nDesigned and orchestrated ML training environments. Ingested large-scale data into ML pipelines. Used containerized training jobs and distributed systems to speed up experimentation. Booking.com Booking.com shows how they use Amazon SageMaker to:\nBuild and train ML models. Analyze data and conduct online experimentation. Improve relevance ranking models and accelerate data-science iteration cycles. SageMaker Immersion Day Workshop A hands-on workshop demonstrating the full ML workflow on SageMaker:\nFeature engineering. Selecting algorithms and training models. Hyperparameter tuning. Deployment to production. Debugging and monitoring model behavior. The workshop simulates a near-real production ML scenario using end-to-end AWS services.\nConclusion This blog provides an overview of tools, architectures, and success stories for building ML solutions on AWS. With managed services, purpose-built hardware, and best-practice guides, AWS enables organizations to move ML models from experimentation into scalable, production-ready systems.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Revolutionizing Telecom Revenue Assurance with AWS AI-Driven Framework",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for official reports, including this notice.\nRevolutionizing Telecom Revenue Assurance: The AWS AI-Driven Framework for Next-Generation Solutions Revenue Assurance (RA) is critical in telecommunications‚Äîensuring accurate billing, validating usage data, reconciling partner settlements, and preventing revenue leakages. Traditional RA methods are often reactive, rely on partial sampling, and struggle to scale with modern telecom services such as 5G, IoT, and complex partner ecosystems. These limitations lead to annual revenue losses estimated between 1%‚Äì3% of total operator revenue.\nThis blog introduces the AWS AI-driven framework designed to modernize RA, enabling proactive detection, automated remediation, and large-scale intelligence across telecom operations.\nAWS AI/ML Framework for Modern Revenue Assurance AWS proposes a three-layer architecture to transform legacy RA platforms into intelligent, scalable systems:\n1. Foundation Layer Powered by Amazon SageMaker, this layer provides:\nHigh-performance ML infrastructure Custom model development and training Purpose-built accelerators Scalable data pipelines for petabyte-level telecom data 2. Middle Layer Amazon Bedrock offers:\nAccess to high-quality Foundation Models (FMs) Fine-tuning capabilities for telecom-specific tasks RAG (Retrieval-Augmented Generation) pipelines Enterprise-grade controls for governance and data security 3. Top Layer Amazon Q acts as:\nAn enterprise AI assistant A system for validating configurations, billing checks, and root-cause analysis A tool to generate summaries, explanations, and workflows for RA processes This layered architecture supports seamless ingestion, processing, and interpretation of large telecom datasets through zero-ETL integrations.\nGenerative AI Approaches for Revenue Assurance AWS integrates Generative AI and ML techniques to overcome the rigidity of rule-based RA systems.\nAI-driven capabilities include: Automated pattern recognition: Detect complex or emerging leak patterns across billing, usage, partner settlements, and mediation data Predictive modeling: Forecast potential leakage risks before they occur Intelligent process automation: Use agentic AI to autonomously validate, reconcile, and recommend corrective actions Explainable anomaly detection: Provide clear reasoning behind identified issues These capabilities result in faster detection, reduced manual work, and significantly improved accuracy.\nIndustry Use Cases \u0026amp; Business Impact 1. Launching new 5G services and network slicing Generative AI analyzes SLA structures, configuration rules, and usage correlations to ensure correctness.\nBenefits:\nUp to ~30% reduction in SLA-related revenue errors ~65% faster validation cycles 2. Partner revenue sharing \u0026amp; settlement AI reads contracts, compares commercial terms, identifies deviations, and accelerates settlement workflows.\nBenefits:\nReduced partner revenue leakage (~3%) 25% improvement in settlement accuracy 3. Real-time usage reconciliation AI models detect micro-leakage, event losses, and data inconsistencies in high-volume 5G/IoT workloads.\nBenefits:\nUp to 70% reduction in CDR-to-bill processing delay 4. Dynamic pricing and catalog assurance AI validates product configurations, pricing logic, and simulations before launch.\nBenefits:\n~35% reduction in revenue leakage from misconfigurations Faster time-to-market 5. Explainable detection with remediation recommendations AI-generated insights help analysts understand anomalies quickly and resolve issues faster.\nBenefits:\nAnalyst productivity improvement (~40%) 60% faster response time Recommended AWS Architecture Components A modern RA system requires integration across billing, CRM, mediation, network analytics, and data lake environments. AWS recommends:\nData Ingestion \u0026amp; Streaming Amazon MSK Amazon Kinesis Data Streams AWS Database Migration Service Secure file transfer \u0026amp; container-based ingestion Processing \u0026amp; Feature Engineering AWS Glue Amazon EMR AWS Lambda Feature Store Amazon Athena Storage \u0026amp; Advanced Analytics Amazon S3 Data Lake Amazon Redshift Amazon Aurora / Amazon RDS Vector databases (Amazon OpenSearch, Amazon Neptune, pgvector) ML/AI Deployment \u0026amp; Orchestration Amazon Bedrock Amazon SageMaker Amazon Q AWS Step Functions Amazon API Gateway Amazon CloudWatch for monitoring This modular architecture allows telecom operators to modernize step-by-step while ensuring scalability and resiliency.\nConclusion The AWS AI-driven framework represents a major shift in telecom Revenue Assurance‚Äîfrom reactive, rule-based methods to intelligent, proactive, automated systems. Using ML, Generative AI, and scalable cloud infrastructure, telecom operators can:\nMinimize revenue leakage Detect issues earlier and automate remediation Support new services like 5G and IoT with confidence Reduce operational costs and manual effort Ensure accurate billing, partner settlement, and product configuration This next-generation approach empowers telecom providers to innovate safely, protect profit margins, and deliver reliable services in an increasingly complex ecosystem.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/3.4-blog4/",
	"title": "Secure Amazon EVS with AWS Network Firewall",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference only. Please do not copy verbatim into official reports, including this notice.\nSecure Amazon Elastic VMware Service (Amazon EVS) with AWS Network Firewall Amazon EVS allows organizations to migrate, run, and scale VMware workloads natively on AWS, using VMware Cloud Foundation (VCF) deployed within an Amazon VPC on bare-metal Amazon EC2 instances. This approach enables rapid migration, seamless scaling, and minimal application refactoring.\n(aws.amazon.com)\nWhen building a hybrid cloud architecture integrating EVS, AWS VPCs, on-premises data centers, and internet connectivity, it is critical to implement consistent network security. Centralized inspection, unified firewall policy management, and consolidated logging/monitoring are essential. AWS recommends AWS Network Firewall as the core inspection and protection layer.\n(aws.amazon.com)\nThis blog explains how to secure an Amazon EVS environment using AWS Network Firewall, applying a centralized inspection model across EVS clusters, VPCs, on-premises environments, and the internet.\nArchitecture Overview AWS Network Firewall acts as a transparent ‚Äúbump-in-the-wire,‚Äù inserted via VPC or Transit Gateway routing, inspecting all traffic without requiring application-level changes. A typical architecture includes: EVS VPC, hosting the underlay VLAN subnets for VCF (management, vMotion, vSAN, NSX overlay, etc.). Workload VPCs (e.g., VPC01) where applications or services run. Ingress VPC with an Application Load Balancer for public traffic. Egress VPC with a NAT Gateway to centralize outbound internet access. Optional on-prem connectivity via AWS Direct Connect + Direct Connect Gateway.\n(aws.amazon.com) All traffic flows‚ÄîEVS ‚Üî VPCs, VPC ‚Üî VPC, VPC ‚Üî on-prem, and VPC ‚Üî Internet‚Äîare inspected by AWS Network Firewall, covering both east-west and north-south traffic.\nKey Benefits Centralized firewall control across EVS, AWS VPCs, on-premises, and internet paths Unified enforcement of security policies across complex hybrid environments Centralized logging and monitoring for audit, compliance, and troubleshooting Implementation Steps (Summary) Create an AWS Transit Gateway\nDisable default association/propagation. Create two route tables:\nPre-inspection Post-inspection\n(aws.amazon.com) Attach EVS, workload VPCs, ingress/egress VPCs, and optional Direct Connect Gateway\nUse the Pre-inspection route table for all attachments.\nDeploy AWS Network Firewall using Transit Gateway integration\nAWS automatically provisions required resources (firewall endpoints, internal routing, etc.).\nSimplifies configuration and reduces operational overhead.\nUpdate route tables\nPre-inspection: Default routing directs traffic into the Firewall for inspection Post-inspection: Routes inspected traffic back to the appropriate VPC destinations Inside each VPC: Update ingress/egress routes so all flows traverse the firewall\n(aws.amazon.com) (Optional) Configure firewall rule groups and logging\nStateful rules (e.g., HTTP/HTTPS, FQDN filtering, east-west segmentation) Send flow logs and alert logs to Amazon CloudWatch Logs or Amazon S3 When configured correctly, all network flows‚Äîacross EVS, AWS VPCs, on-prem, and the internet‚Äîare monitored and protected.\nAdvantages Over Traditional Security Approaches No application-level changes required; firewall operates transparently at the network layer Centralized management simplifies policy enforcement across multiple VPCs and VMware clusters Scales dynamically using AWS-managed infrastructure Unified logs streamline compliance, auditing, and incident analysis Conclusion This blog demonstrates how AWS Network Firewall can secure Amazon EVS by providing a centralized, scalable network inspection model. Traffic between EVS, workload VPCs, on-premises environments, and the internet is consistently monitored and enforced.\nOrganizations can migrate VMware workloads to AWS confidently‚Äîpreserving the benefits of cloud scalability while maintaining strong network security, unified control, and comprehensive visibility.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nWeek 2 Objectives Improve operational capabilities: autoscaling, monitoring, DNS, CDN, CLI, and NoSQL / caching databases. Practice deploying systems with basic scaling \u0026amp; high availability, using CloudWatch for monitoring and Route 53 for DNS configuration. Get familiar with DynamoDB, ElastiCache, CloudFront, Lambda@Edge, and basic Windows/Directory administration on AWS. Tasks to be carried out this week: Day Task (details) Start Date Completion Date Reference Material 2 Scaling Applications with EC2 Auto Scaling - Create Launch Template/Configuration, Auto Scaling Group (ASG), scale-out/scale-in policies; test by simulating increased load. TBD TBD cloudjourney.awsstudygroup.com. 2 Monitoring with Amazon CloudWatch - Create CloudWatch Alarm, custom metrics, log group; configure dashboard to monitor CPU, network, health check. TBD TBD cloudjourney.awsstudygroup.com. 3 Hybrid DNS Management with Amazon Route 53 - Create hosted zone, record sets (A/CNAME/ALIAS), health check; combine internal DNS with public DNS if needed. TBD TBD cloudjourney.awsstudygroup.com. 3 Command Line Operations with AWS CLI - Install \u0026amp; configure AWS CLI, practice EC2/S3/RDS/DynamoDB commands, small scripts to automate frequent tasks. TBD TBD cloudjourney.awsstudygroup.com. 4 NoSQL Database Essentials (Amazon DynamoDB) - Create table, design partition key \u0026amp; sort key, read/write items, test throughput (read/write capacity / on-demand). TBD TBD cloudjourney.awsstudygroup.com. 4 In-Memory Caching with Amazon ElastiCache - Deploy Redis/Memcached; integrate caching for applications; configure cluster replication (if needed) and test reduced latency. TBD TBD cloudjourney.awsstudygroup.com. 5 Networking on AWS Workshop - Advanced exercises on VPC, peering, overview of VPN/Direct Connect, review flow logs and network troubleshooting. TBD TBD cloudjourney.awsstudygroup.com. 5 Content Delivery with Amazon CloudFront - Create distribution, origin (S3/ALB), invalidation, verify TTL; use CloudFront to accelerate static assets. TBD TBD cloudjourney.awsstudygroup.com. 6 Edge Computing with CloudFront and Lambda@Edge - Write small Lambda@Edge functions to modify headers/redirect; understand edge compute use cases. TBD TBD cloudjourney.awsstudygroup.com. 6 Windows Workloads on AWS - Deploy EC2 Windows instance, license model, RDP access, basic patching \u0026amp; backup configuration. TBD TBD cloudjourney.awsstudygroup.com. 7 Directory Services (AWS Managed Microsoft AD) - Initialize AWS Managed Microsoft AD, domain join EC2 Windows, test authentication and simple group policy. TBD TBD cloudjourney.awsstudygroup.com. 7 Building Highly Available Web Applications - Design multi-AZ for EC2/RDS, load balancing (ALB), health checks, automatic scaling with ASG, use CloudFront + Route 53 to enhance HA and performance. TBD TBD cloudjourney.awsstudygroup.com. Week 2 Outcomes: Understand and configure Auto Scaling for EC2 applications; able to simulate scale-out/scale-in. Set up CloudWatch metrics/alarms and dashboards for basic monitoring; know how to collect logs. Configure DNS with Route 53, use CloudFront to optimize content delivery; experiment with simple Lambda@Edge functions. Deploy DynamoDB and ElastiCache; understand trade-offs between RDBMS / NoSQL / cache. Perform basic Windows workload administration and test AWS Managed Microsoft AD for authentication. "
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog ‚Äì Migrate to AWS",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: This is a reference template. Do not copy it verbatim for any official report submission.\nObjectives for this week: Understand an overview of the services that support Migration on AWS. Learn the workflow for migrating systems from On-premises or another Cloud to AWS. Know the roles of each Migration service (DMS, SCT, VM Import/Export, Elastic Disaster Recovery‚Ä¶). Prepare fundamentals for next week‚Äôs Migration hands-on exercises. Tasks to be carried out this week: Day Task Start Date Completion Date References 2 - Read an overview of the Migrate to AWS category - Identify the key groups of Migration services dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about VM Migration with AWS VM Import/Export + How to import VMs + Supported formats dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 4 - Study Database Migration with: + AWS Database Migration Service (DMS) + Schema Conversion Tool (SCT) dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 5 - Read and take notes on AWS Elastic Disaster Recovery (DR) + How it works + Mechanisms for reducing downtime dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 6 - Research the ‚ÄúComing Soon‚Äù services: + AWS Migration Hub + AWS Application Migration Service + AWS Migration Evaluator + AWS Application Discovery Service dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ Achievements: Gained an understanding of the entire set of services in the Migrate to AWS group, including:\nAWS VM Import/Export AWS Database Migration Service (DMS) Schema Conversion Tool (SCT) AWS Elastic Disaster Recovery AWS Application Migration‚Äìrelated services (Coming Soon) Clearly understood the migration categories:\nVM Migration: Moving virtual machines. Database Migration: Migrating databases + converting schemas. Disaster Recovery: Preparing systems for recovery after incidents. Full Application Migration: (upcoming services). Understood the purpose of each service:\nVM Import/Export ‚Üí Move VMs from VMware/Hyper-V to EC2. DMS ‚Üí Migrate data with minimal downtime. SCT ‚Üí Convert schemas when changing engines (Oracle ‚Üí Aurora‚Ä¶). Elastic Disaster Recovery ‚Üí Create ready-to-failover replicas. Migration Hub ‚Üí Track the entire Migration project. Application Migration Service ‚Üí Automatically convert On-prem machines to EC2. Took notes on the use cases of each service and the differences between Migration \u0026amp; Disaster Recovery.\nPrepared materials for next week‚Äôs Migration practice sessions.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog ‚Äì Optimizing the System on AWS (Operations)",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: This is only a reference template. Do not copy it verbatim for your official submission.\nObjectives for this week: Understand system optimization methods when operating on AWS. Grasp the key Operations tool groups: Monitoring, Automation, IaC, Resource Optimization, Cost Management. Practice with CloudWatch, Systems Manager, IAM, Resource Tags, and Amazon EBS automation. Be ready to optimize performance, cost, and security in real AWS environments. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Read the overview of Optimizing the System on AWS - Explain the role of Operations in AWS dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn Serverless Automation with AWS Lambda - Advanced Monitoring with CloudWatch \u0026amp; Grafana dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 4 - CloudWatch Advanced Workshop (Logs, Metrics, Alarms) - Manage resources with Tags and Resource Groups dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 5 - IAM \u0026amp; Resource Tags for fine-grained access control - Systems Management with AWS Systems Manager - Server access via Session Manager dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn Infrastructure as Code with AWS CloudFormation - Learn AWS CDK (Essentials + Advanced) - Resource Optimization with EC2 dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ 7 - Network Monitoring with VPC Flow Logs - Cost \u0026amp; Usage Management - EBS Snapshot Automation with Amazon EBS DLM - Anomaly Detection for EBS Backups dd/mm/2025 dd/mm/2025 https://cloudjourney.awsstudygroup.com/ Achievements: Understood the role of Operations in optimizing systems:\nAutomation (Lambda, Systems Manager) Advanced Monitoring (CloudWatch, Grafana) Standardized resource management using Tags Infrastructure as Code (CloudFormation, CDK) Cost \u0026amp; performance optimization (EC2 right-sizing, billing tools) Reliability \u0026amp; backup optimization (EBS, DLM, Snapshot Automation) Gained detailed understanding of each key service this week:\nAWS Lambda: Automations, event-driven tasks CloudWatch \u0026amp; Grafana: Dashboards, metrics, logs, anomaly detection IAM + Resource Tags: Fine-grained permissions Systems Manager + Session Manager: Server access \u0026amp; management without SSH CloudFormation \u0026amp; AWS CDK: Repeatable, version-controlled infrastructure deployment EC2 Resource Optimization: Right-sizing, unused instance checks VPC Flow Logs: Network traffic monitoring for security \u0026amp; debugging Cost and Usage Management: Cost tracking \u0026amp; alerts Amazon EBS Lifecycle Manager: Automated scheduled snapshots Completed notes and analysis on system optimization:\nPerformance optimization Cost optimization Security optimization Administration \u0026amp; automation optimization Prepared foundational knowledge for next week: Security Optimization / Performance Optimization / Cost Optimization.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Creating Interface VPC Endpoint for S3 Access",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will deploy an S3 Interface VPC Endpoint within the cloud VPC. This endpoint enables private connectivity from your simulated on-premises network to Amazon S3, eliminating the need for internet gateways or NAT devices.\nWhy Interface Endpoints for Hybrid Access?\nInterface VPC Endpoints create elastic network interfaces (ENIs) with private IP addresses in your VPC. This architecture provides several advantages for hybrid cloud scenarios:\nPrivate IP Addressing: On-premises systems can resolve S3 service names to private IPs within your VPC CIDR range DNS Integration: Works seamlessly with Route 53 Resolver for hybrid DNS resolution Security: Traffic flows through private connections (VPN/Direct Connect) without traversing the public internet PrivateLink Technology: Leverages AWS PrivateLink for secure, scalable service access Fine-Grained Access Control: Apply security groups and endpoint policies for granular access management Interface vs Gateway Endpoints: While you previously used a Gateway Endpoint (which modifies route tables), Interface Endpoints use ENIs with private IPs. This makes them suitable for on-premises access through VPN/Direct Connect, as they can be reached via standard IP routing.\nStep-by-Step Interface Endpoint Creation Step 1: Navigate to VPC Endpoints Console\nOpen the Amazon VPC Console In the left navigation menu, click Endpoints Click the Create endpoint button in the top-right corner Step 2: Configure Endpoint Basic Settings\nIn the Create Endpoint configuration page:\nName your endpoint (optional but recommended):\nEnter a descriptive name: S3-Interface-Endpoint-HybridAccess This helps identify the endpoint\u0026rsquo;s purpose in production environments Select Service Category:\nChoose AWS services This filters the service list to AWS-managed endpoints Naming Convention Best Practice: Use descriptive names that indicate the service, endpoint type, and purpose, e.g., S3-Interface-CloudVPC or S3-InterfaceEP-Production.\nStep 3: Locate and Select the S3 Interface Service\nSearch for S3 service:\nIn the Services search box, type: S3 Press Enter or click the search icon Identify the correct service:\nLook for the service named: com.amazonaws.us-east-1.s3 Verify the Type column shows \u0026ldquo;Interface\u0026rdquo; (not Gateway) Click the radio button to select this service Critical Selection: AWS offers both Gateway and Interface endpoint types for S3. Ensure you select the Interface type. Gateway endpoints won\u0026rsquo;t work for on-premises access scenarios because they only modify VPC route tables.\nStep 4: Configure VPC and DNS Settings\nSelect Target VPC: From the VPC dropdown, select VPC Cloud Do NOT select \u0026ldquo;VPC On-prem\u0026rdquo; - the endpoint must be in the cloud VPC VPC Selection is Critical: The Interface Endpoint must be created in VPC Cloud, not VPC On-prem. On-premises systems will access this endpoint through the VPN tunnel you configured earlier.\nConfigure DNS Settings: Expand the Additional settings section Uncheck \u0026ldquo;Enable DNS name\u0026rdquo; (disable it) Why? You\u0026rsquo;ll manually configure DNS using Route 53 Private Hosted Zones for more control DNS Configuration Choice: In this workshop, you\u0026rsquo;re using Route 53 Resolver with Private Hosted Zones for DNS resolution (configured in section 5.4.1). This provides greater flexibility and mirrors enterprise hybrid DNS architectures. Alternatively, enabling \u0026ldquo;Enable DNS name\u0026rdquo; would automatically create Route 53 private DNS records, but we want explicit control for learning purposes.\nStep 5: Select Availability Zones and Subnets\nConfigure High Availability:\nSelect 2 subnets in different Availability Zones: Availability Zone: us-east-1a Choose the corresponding private subnet in VPC Cloud Availability Zone: us-east-1b Choose the corresponding private subnet in VPC Cloud Why Multiple AZs?\nDeploying Interface Endpoint ENIs in multiple Availability Zones provides:\nHigh Availability: If one AZ experiences issues, traffic can route through the other Fault Tolerance: Automatic failover between ENIs in different AZs Geographic Redundancy: Reduces latency by having endpoints in multiple locations Production Best Practice: AWS recommends multi-AZ deployment for critical services Best Practice: Always deploy Interface Endpoints across at least 2 Availability Zones for production workloads. This ensures your on-premises applications maintain S3 connectivity even during AZ failures.\nStep 6: Apply Security Group\nSelect Security Group: In the Security groups dropdown, choose SGforS3Endpoint This security group was created by the CloudFormation template It contains inbound rules allowing HTTPS (port 443) traffic from on-premises CIDR Security Group Purpose:\nThe SGforS3Endpoint security group controls which sources can communicate with the Interface Endpoint ENIs:\nInbound Rules:\r- Protocol: TCP\r- Port: 443 (HTTPS)\r- Source: VPC On-prem CIDR (e.g., 10.1.0.0/16)\r- Purpose: Allow S3 API calls from on-premises systems Security Layer: Security groups act as virtual firewalls for your Interface Endpoint. Even though traffic comes through a VPN, the security group provides an additional layer of control, following AWS defense-in-depth principles.\nStep 7: Configure Endpoint Policy\nPolicy Configuration:\nLeave the Policy set to Full access (default) This allows all S3 actions through the endpoint Understanding Endpoint Policies:\nEndpoint policies control what AWS API actions can be performed through the endpoint They work in conjunction with IAM policies (both must allow an action) For this workshop, full access simplifies testing Click Create endpoint\nProduction Consideration: In production environments, use restrictive endpoint policies following the principle of least privilege. For example, limit to specific S3 buckets or actions like s3:GetObject and s3:PutObject only.\nExample Restrictive Policy:\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::my-company-data/*\u0026#34;, \u0026#34;arn:aws:s3:::my-company-data\u0026#34; ] } ] } Verify Endpoint Creation Check Endpoint Status:\nThe endpoint creation typically takes 2-3 minutes Refresh the Endpoints page periodically Wait until the Status changes from Pending to Available Once available, note the Endpoint ID (format: vpce-xxxxxxxxxxxxxxxxx) Inspect Endpoint Details:\nClick on the newly created endpoint to view:\nDNS names: Private DNS names assigned to the endpoint Subnets: Verify both AZs are listed with their ENI IDs Network interfaces: Each subnet has a dedicated ENI with a private IP Security groups: Confirm SGforS3Endpoint is attached Record Private IP Addresses:\nIn the Subnets section, note the private IP addresses assigned to each ENI. You\u0026rsquo;ll use these in DNS configuration (next step).\nUnderstanding What You Created Architecture Overview:\nYou\u0026rsquo;ve deployed an Interface VPC Endpoint with the following components:\nTwo Elastic Network Interfaces (ENIs):\nOne ENI in us-east-1a subnet with a private IP (e.g., 10.0.1.100) One ENI in us-east-1b subnet with a private IP (e.g., 10.0.2.100) Security Group Protection:\nENIs protected by SGforS3Endpoint security group Only allows HTTPS traffic from on-premises CIDR Private Connectivity Path:\nOn-Premises Systems ‚Üí VPN Tunnel ‚Üí VPC Cloud ‚Üí Interface Endpoint ENI ‚Üí AWS PrivateLink ‚Üí S3 Service How On-Premises Access Works:\nWhen an on-premises application makes an S3 API call:\nDNS query for s3.amazonaws.com is sent to Outbound Resolver Outbound Resolver forwards through VPN to Inbound Resolver Inbound Resolver queries Private Hosted Zone (to be configured next) Private Hosted Zone returns Interface Endpoint private IP Application sends HTTPS request to private IP through VPN Request reaches Interface Endpoint ENI PrivateLink routes request to S3 service Response follows reverse path back to on-premises Congratulations! You\u0026rsquo;ve successfully created an S3 Interface VPC Endpoint for hybrid cloud access. This endpoint provides a secure, private connection from your on-premises environment to Amazon S3, eliminating internet exposure and enabling enterprise-grade hybrid architectures.\nNext Steps With the Interface Endpoint created, you\u0026rsquo;ll proceed to:\nConfigure DNS records in Route 53 Private Hosted Zone to resolve S3 DNS names to endpoint IPs Test connectivity from the on-premises EC2 instance to S3 through the private endpoint Verify traffic flow using VPC Flow Logs and endpoint metrics Validate security by confirming all traffic stays within AWS private networks The Interface Endpoint is now ready to serve as your private gateway to S3 for hybrid workloads!\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nRequired IAM Permissions Before starting this workshop, you must ensure your AWS IAM user or role has sufficient permissions to create and manage the necessary resources. Attach the following custom IAM policy to your user account or assume a role with these permissions.\nImportant Security Notes:\nReview all permissions before applying to understand what resources will be created These permissions are required for workshop deployment and cleanup operations After completing the workshop, consider removing these permissions if they\u0026rsquo;re no longer needed For production environments, always follow the principle of least privilege {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Policy Coverage: This IAM policy grants permissions across multiple AWS services essential for the workshop:\nEC2 \u0026amp; VPC Networking: Create and manage VPCs, subnets, route tables, security groups, Transit Gateway, VPN connections, and VPC endpoints CloudFormation: Deploy and manage infrastructure as code stacks IAM: Create roles and instance profiles for EC2 instances S3: Create buckets and manage objects for workshop resources Lambda \u0026amp; CloudWatch: Deploy serverless functions and monitor resources Route53: Configure DNS and resolver endpoints SSM (Systems Manager): Enable secure instance access via Session Manager Secrets Manager: Store and retrieve sensitive configuration data Environment Setup Using Infrastructure as Code Workshop Region: This lab is designed for the US East (N. Virginia) region (us-east-1). Ensure you select this region before proceeding.\nAutomated Deployment Overview: Instead of manually creating each resource, we\u0026rsquo;ll use AWS CloudFormation to automate the entire infrastructure setup. This Infrastructure as Code (IaC) approach provides:\nConsistency: All participants work with identical configurations Speed: Deploy complex multi-resource environments in minutes Repeatability: Easy to recreate or clean up the environment Best Practices: Pre-configured with AWS recommended settings Deployment Steps:\nLaunch the CloudFormation Stack:\nClick this link to open the CloudFormation quick-create console: Deploy Workshop Infrastructure The template URL is pre-populated with the workshop configuration Stack name is automatically set to PLCloudSetup Review Stack Parameters: Review the pre-configured parameters (all defaults are optimized for this workshop):\nAcknowledge IAM Resource Creation: Scroll to the bottom of the page Check both acknowledgment boxes: ‚úì \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources.\u0026rdquo; ‚úì \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources with custom names.\u0026rdquo; Click Create stack Monitor Deployment Progress: CloudFormation will begin provisioning resources Expected deployment time: approximately 15-20 minutes You can monitor progress in the CloudFormation console under the \u0026ldquo;Events\u0026rdquo; tab The stack uses nested stacks to organize resource creation logically What Gets Created: The CloudFormation template automatically provisions:\n2 Virtual Private Clouds (VPCs): One simulating cloud environment, one simulating on-premises Multiple Subnets: Public and private subnets across availability zones AWS Transit Gateway: Central hub for VPC connectivity Site-to-Site VPN: Pre-configured VPN connection between VPCs 3 EC2 Instances: Test instances in different network segments Security Groups: Pre-configured for workshop traffic IAM Roles: Instance profiles for EC2 instances Route Tables: Properly configured routing between environments Verify Deployment:\nOnce the stack status shows CREATE_COMPLETE, verify the resources:\nCheck VPCs Created: Navigate to VPC Dashboard and confirm two VPCs are present: Cloud VPC (for AWS cloud resources) On-Premises VPC (simulating datacenter) Check EC2 Instances: Navigate to EC2 Dashboard and verify three instances are running: Cloud Test Instance (in Cloud VPC) On-Prem Test Instance (in On-Prem VPC) VPN Gateway Instance (for Site-to-Site connectivity) Troubleshooting:\nIf stack creation fails, check the \u0026ldquo;Events\u0026rdquo; tab for error messages Ensure your IAM user has all required permissions from the policy above Verify you\u0026rsquo;re in the us-east-1 region Check AWS service quotas for EC2, VPC, and CloudFormation if you encounter limits You\u0026rsquo;re now ready to proceed with the workshop exercises!\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Testing Gateway Endpoint Connectivity",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nNow that you\u0026rsquo;ve created the Gateway Endpoint, it\u0026rsquo;s time to verify that it works correctly. In this section, you\u0026rsquo;ll create an S3 bucket, connect to an EC2 instance in your VPC, and upload a test file to S3 - all without traffic leaving the AWS network.\nTesting Objectives:\nVerify private connectivity to S3 through the Gateway Endpoint Confirm that traffic does not traverse the public internet Demonstrate successful S3 operations from a private subnet Validate route table configuration Step 1: Create an S3 Bucket for Testing First, you need an S3 bucket to test uploads through the Gateway Endpoint.\nNavigate to S3 Console: Open the S3 Management Console Click the Create bucket button Configure Bucket Settings:\nBucket Name:\nEnter a globally unique name (S3 bucket names must be unique across all AWS accounts) Suggested format: gateway-endpoint-test-\u0026lt;your-initials\u0026gt;-\u0026lt;random-number\u0026gt; Example: gateway-endpoint-test-jd-20241207 AWS Region:\nEnsure the region is US East (N. Virginia) us-east-1 This must match the region where your VPC and Gateway Endpoint are deployed Object Ownership:\nLeave as default (ACLs disabled - Recommended) Block Public Access settings:\nLeave all options checked (default) This ensures the bucket remains private Bucket Versioning:\nLeave as default (Disabled) Default Encryption:\nLeave as default (Server-side encryption with Amazon S3 managed keys - SSE-S3) Create the Bucket: Scroll to the bottom Click Create bucket Verify Creation: You should see a success message Your new bucket appears in the buckets list Security Best Practice: Notice that the bucket is created with all public access blocked by default. This aligns with AWS security best practices. Access will only be possible through the VPC endpoint and with proper IAM permissions.\nStep 2: Connect to EC2 Instance Using Session Manager You\u0026rsquo;ll use AWS Systems Manager Session Manager to securely connect to the EC2 instance without requiring SSH keys, bastion hosts, or open inbound ports.\nWhat is AWS Session Manager? Session Manager is a fully managed Systems Manager capability that provides:\nSecure Access: Browser-based shell access without opening inbound ports Audit Trail: All sessions are logged in CloudWatch Logs No SSH Keys: No need to manage or distribute SSH keys Cross-Platform: Works with Linux and Windows instances Architecture Context: The EC2 instance you\u0026rsquo;ll connect to is running in a private subnet within \u0026ldquo;VPC Cloud\u0026rdquo;. It has no direct internet access - connectivity to AWS services (including Systems Manager) is provided through the SSM VPC Interface Endpoints that were created during CloudFormation deployment.\nOpen Systems Manager Console: In the AWS Management Console search bar, type Systems Manager Press Enter or click on the Systems Manager service Navigate to Session Manager: In the left navigation pane, locate Node Management section Click on Session Manager Start a New Session: Click the Start session button From the list of available instances, select the instance named Test-Gateway-Endpoint Instance Details: This EC2 instance is deployed in a private subnet of \u0026ldquo;VPC Cloud\u0026rdquo; and is configured with an IAM instance profile that grants necessary permissions for Session Manager and S3 access. The instance will be used to verify that S3 traffic flows through the Gateway Endpoint you created, rather than the public internet.\nSession Established: Session Manager opens a new browser tab with a shell prompt You should see a prompt like: sh-4.2$ or similar This indicates you\u0026rsquo;re successfully connected to the EC2 instance Connectivity Verification: The fact that Session Manager works confirms that the SSM Interface Endpoints are functioning correctly. If you couldn\u0026rsquo;t connect, it would indicate an issue with the Interface Endpoints or IAM permissions.\nStep 3: Create and Upload Test File to S3 Now you\u0026rsquo;ll create a test file on the EC2 instance and upload it to S3 through the Gateway Endpoint.\nNavigate to Home Directory:\nExecute the following command to switch to the ssm-user\u0026rsquo;s home directory:\ncd ~ Why this step? Working in the home directory ensures you have proper write permissions and a clean working environment.\nCreate a Test File:\nGenerate a 1GB test file using the following command:\nfallocate -l 1G testfile.xyz Command Explanation:\nfallocate: Efficiently allocates space for a file -l 1G: Specifies file size of 1 gigabyte testfile.xyz: The filename for our test file This creates a 1GB file almost instantly by allocating disk space without writing actual data. We use a large file to make the upload more observable and to demonstrate that the Gateway Endpoint can handle substantial data transfers.\nUpload File to S3 Bucket:\nUse the AWS CLI to upload the file to your S3 bucket:\naws s3 cp testfile.xyz s3://YOUR-BUCKET-NAME/ Important: Replace YOUR-BUCKET-NAME with the actual name of the bucket you created in Step 1.\nWhat\u0026rsquo;s Happening Behind the Scenes:\nThe EC2 instance makes an API call to S3 The VPC route table directs S3 traffic to the Gateway Endpoint (prefix list destination) Traffic flows through the Gateway Endpoint to S3, staying within the AWS network No internet gateway or NAT gateway is used Success Indicator: If the upload completes successfully, you\u0026rsquo;ll see output showing the file transfer progress and completion. This confirms that:\nThe Gateway Endpoint is configured correctly Route table entries are working as expected The EC2 instance has proper IAM permissions for S3 Private connectivity to S3 is functioning Terminate the Session: Type exit or close the browser tab The session will be cleanly terminated Step 4: Verify Object in S3 Bucket Finally, confirm that the file was successfully uploaded by checking the S3 console.\nReturn to S3 Console:\nNavigate back to the S3 Management Console Open Your Bucket:\nClick on the name of the bucket you created earlier Verify File Presence:\nYou should see testfile.xyz listed in the Objects tab The file size should show approximately 1 GB Note the upload timestamp Verification Checklist:\n‚úì File testfile.xyz appears in bucket ‚úì File size matches (1 GB) ‚úì Upload timestamp is recent ‚úì No errors in S3 console Understanding What You Just Accomplished Network Path Verification: In this test, you successfully:\nCreated Private Connectivity: The Gateway Endpoint enabled private connectivity from your VPC to S3 Bypassed Public Internet: The upload did not traverse the public internet - it stayed entirely within AWS\u0026rsquo;s network backbone Used Route-Based Routing: The route table automatically directed S3 traffic to the Gateway Endpoint based on the S3 prefix list Demonstrated Cost Savings: By using a Gateway Endpoint instead of a NAT Gateway, you avoided: NAT Gateway hourly charges NAT Gateway data processing charges Potential internet gateway data transfer costs Traffic Flow Diagram:\nEC2 Instance ‚Üí VPC Route Table ‚Üí Gateway Endpoint ‚Üí S3 Service\r(Private Subnet) (Routes S3 to vpce-xxx) (No internet) (AWS Network) Key Differences from Internet-Based Access:\nWithout Gateway Endpoint: EC2 ‚Üí NAT Gateway ‚Üí Internet Gateway ‚Üí Public Internet ‚Üí S3 With Gateway Endpoint: EC2 ‚Üí Gateway Endpoint ‚Üí S3 (all within AWS network) Performance Benefits: Gateway Endpoints not only save costs but also typically provide better performance than internet-based access due to:\nLower latency (direct AWS network path) Higher available bandwidth No NAT Gateway bottleneck Dedicated AWS backbone infrastructure Section Summary Congratulations! You\u0026rsquo;ve successfully completed the Gateway Endpoint testing exercise.\nWhat You Learned:\n‚úì How to create and configure S3 buckets with security best practices ‚úì Using AWS Systems Manager Session Manager for secure instance access ‚úì Uploading objects to S3 through a Gateway VPC Endpoint ‚úì Verifying private connectivity without internet gateway dependency ‚úì Understanding the traffic flow in a VPC with Gateway Endpoints Key Takeaways:\nGateway Endpoints provide cost-effective private connectivity to S3 and DynamoDB No additional charges for using Gateway Endpoints - you only pay for S3 storage and requests Traffic remains private - never exposed to the public internet Automatic routing through route table entries - no complex configuration needed Scalable and highly available - AWS manages the endpoint infrastructure Next Steps: In the following sections, you\u0026rsquo;ll explore:\nCreating Interface VPC Endpoints for other AWS services Implementing VPC Endpoint policies for fine-grained access control Testing connectivity from on-premises (simulated) environments Understanding the differences between Gateway and Interface Endpoints "
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you need to summarize the contents of the workshop that you plan to conduct.\nIoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement What‚Äôs the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two parts‚Äîsetting up weather edge stations and building the weather platform‚Äîeach following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Testing Interface Endpoint Connectivity from On-Premises",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will validate that the Interface VPC Endpoint is working correctly by testing S3 connectivity from the simulated on-premises environment. This demonstrates how enterprise applications running in corporate datacenters can securely access AWS services through private connections.\nTesting Objectives:\nRetrieve the Interface Endpoint DNS name for S3 API calls Connect to an EC2 instance simulating an on-premises server Upload data to S3 using the private endpoint connection Verify successful data transfer through the hybrid architecture Why This Test Matters:\nThis validation confirms that your hybrid networking configuration is functioning correctly:\nDNS resolution from on-premises to endpoint private IPs VPN routing directing traffic to the cloud VPC Interface Endpoint ENIs receiving and processing S3 API requests Security groups allowing proper traffic flow End-to-end private connectivity without internet exposure Part 1: Retrieve Interface Endpoint DNS Information Before testing connectivity, you need to obtain the endpoint-specific DNS name that on-premises systems will use to reach S3.\nStep 1: Access VPC Endpoints Console\nOpen the Amazon VPC Console In the left navigation pane, click Endpoints You should see your recently created Interface Endpoint Step 2: Locate Endpoint DNS Names\nFind your endpoint:\nLook for the endpoint you created in the previous section It should have a name like S3-Interface-Endpoint-HybridAccess or similar Click on the Endpoint ID (format: vpce-xxxxxxxxx) to view details Access DNS configuration:\nIn the endpoint details page, scroll to the Details tab Look for the DNS names section Identify the Regional DNS Name:\nYou\u0026rsquo;ll see multiple DNS names listed Copy the first DNS name (the regional endpoint DNS name) Format: vpce-xxxxxxxxx.s3.us-east-1.vpce.amazonaws.com Save this to a text editor - you\u0026rsquo;ll need it shortly DNS Name Types: Interface Endpoints provide several DNS names:\nRegional DNS name: Works across all AZs (recommended for general use) Zonal DNS names: Specific to each AZ where you deployed ENIs Private DNS name: Only works if you enabled private DNS (which we didn\u0026rsquo;t) For this test, use the regional DNS name as it provides automatic failover between AZs.\nUnderstanding the DNS Name Structure:\nThe regional DNS name follows this pattern:\nvpce-\u0026lt;endpoint-id\u0026gt;.s3.\u0026lt;region\u0026gt;.vpce.amazonaws.com This DNS name resolves to the private IP addresses of the Interface Endpoint ENIs deployed in your VPC subnets.\nPart 2: Connect to Simulated On-Premises Server Now you\u0026rsquo;ll connect to an EC2 instance that simulates an on-premises server in your corporate datacenter.\nStep 1: Open AWS Systems Manager Session Manager\nNavigate to Systems Manager:\nIn the AWS Console search bar at the top, type Session Manager Select Session Manager from the results (under Systems Manager) Access Session Manager:\nYou\u0026rsquo;ll be taken to the Session Manager start page This service provides secure shell access without requiring SSH keys or bastion hosts Session Manager Advantages: Session Manager provides secure browser-based shell access to EC2 instances without:\nOpening inbound SSH ports (improved security) Managing SSH keys or credentials Deploying bastion hosts Configuring security group SSH rules Perfect for enterprise environments with strict security requirements.\nStep 2: Start Interactive Session\nInitiate connection:\nClick the Start session button You\u0026rsquo;ll see a list of available EC2 instances Select the on-premises instance:\nLook for an instance named Test-Interface-Endpoint This EC2 instance runs in \u0026ldquo;VPC On-prem\u0026rdquo; subnet It simulates a server in your corporate datacenter Click the radio button next to this instance Start the session:\nClick Start session button at the bottom Session Manager will open a new browser tab with a shell prompt You should see: sh-4.2$ or similar prompt Instance Context: The Test-Interface-Endpoint EC2 instance is configured to:\nRun in the \u0026ldquo;VPC On-prem\u0026rdquo; private subnet Route traffic through the VPN Gateway you configured Use the Route 53 Outbound Resolver for DNS queries Simulate an on-premises application server accessing AWS services Part 3: Prepare Test Data for Upload You\u0026rsquo;ll now create a test file to upload to S3, demonstrating data transfer through the private endpoint.\nStep 1: Navigate to User Home Directory\nIn the Session Manager terminal, execute:\ncd ~ This changes to the ssm-user home directory where you have write permissions.\nStep 2: Create a Test File\nGenerate a 1GB test file to simulate a realistic data transfer:\nfallocate -l 1G onprem-data.dat Command Breakdown:\nfallocate: Efficiently creates a file of specified size -l 1G: Allocates 1 gigabyte of space onprem-data.dat: Output filename This creates a sparse file instantly without writing actual data (perfect for testing).\nStep 3: Verify File Creation\nConfirm the file was created:\nls -lh onprem-data.dat You should see output showing a 1.0G file.\nFile Size Selection: Using a 1GB file provides:\nMeasurable transfer time to observe network performance Sufficient size to verify throughput through VPN and endpoint Realistic representation of enterprise data transfers Quick enough for workshop purposes (full upload in seconds over AWS network) Part 4: Upload Data Through Interface Endpoint Now comes the critical test: uploading data to S3 using the Interface Endpoint you created.\nStep 1: Construct the Endpoint URL\nFor Interface Endpoints, you must specify a custom endpoint URL. The format is:\nhttps://bucket.\u0026lt;regional-dns-name\u0026gt; Build your URL:\nStart with: https://bucket. Append the regional DNS name you copied earlier Example result: https://bucket.vpce-0a1b2c3d4e5f.s3.us-east-1.vpce.amazonaws.com DNS Name Format: When copying the regional DNS name from the VPC console:\nDo NOT include the leading asterisk (*.) if shown Use only the portion: vpce-xxxxxxx.s3.us-east-1.vpce.amazonaws.com The bucket. prefix is added in the endpoint URL Step 2: Identify Your S3 Bucket\nYou need the name of the S3 bucket created earlier in section 5.3 (Gateway Endpoint testing):\nFormat: Something like my-endpoint-test-bucket-123456 If you don\u0026rsquo;t remember, check the S3 console in another browser tab Step 3: Execute S3 Upload Command\nIn the Session Manager terminal, run:\naws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; onprem-data.dat s3://\u0026lt;your-bucket-name\u0026gt; Replace placeholders:\n\u0026lt;Regional-DNS-Name\u0026gt;: Your actual endpoint DNS name (without asterisk) \u0026lt;your-bucket-name\u0026gt;: Your actual S3 bucket name Example with real values:\naws s3 cp --endpoint-url https://bucket.vpce-0a1b2c3d4e.s3.us-east-1.vpce.amazonaws.com onprem-data.dat s3://my-endpoint-test-bucket-123456 Understanding the Command:\naws s3 cp: AWS CLI command to copy files to/from S3 --endpoint-url: Critical parameter - directs the S3 API call to the Interface Endpoint instead of public S3 endpoint onprem-data.dat: Source file (local file on the instance) s3://\u0026lt;bucket-name\u0026gt;: Destination S3 bucket Why \u0026ndash;endpoint-url is Required: By default, AWS CLI uses the public S3 endpoint (s3.amazonaws.com). The --endpoint-url parameter overrides this to use your Interface Endpoint\u0026rsquo;s private DNS name, ensuring traffic routes through the VPN and private endpoint rather than the internet.\nStep 4: Monitor Upload Progress\nYou should see output similar to:\nupload: ./onprem-data.dat to s3://my-bucket/onprem-data.dat\rCompleted 1.0 GiB/1.0 GiB (50.0 MiB/s) The upload should complete within seconds given the high-speed AWS backbone network.\nWhat Just Happened:\nBehind the scenes, your data traveled through this path:\nEC2 On-Prem Instance ‚Üí Route Table Lookup ‚Üí VPN Gateway EC2 ‚Üí IPsec VPN Tunnel ‚Üí Transit Gateway ‚Üí VPC Cloud ‚Üí Interface Endpoint ENI (Private IP) ‚Üí AWS PrivateLink ‚Üí S3 Service All traffic remained on the AWS private network - no internet exposure!\nPart 5: Verify Successful Upload in S3 Console Finally, confirm the data arrived successfully in your S3 bucket.\nStep 1: Navigate to S3 Console\nOpen the Amazon S3 Console You\u0026rsquo;ll see a list of all your S3 buckets Step 2: Access Your Bucket\nLocate your bucket:\nFind the bucket you used in the upload command Click on the bucket name to view its contents View objects:\nYou should now see the file onprem-data.dat listed Check the Size column - it should show 1.0 GB Note the Last modified timestamp - should be recent Step 3: Inspect Object Details (Optional)\nClick on the object name to view detailed properties:\nStorage class: Likely Standard Server-side encryption: May show default encryption Metadata: Any custom metadata added Tags: Any tags applied during upload Test Successful! Your on-premises simulated environment successfully uploaded data to S3 through:\nRoute 53 Outbound Resolver (DNS query from on-prem) VPN Tunnel (network connectivity) Route 53 Inbound Resolver (DNS resolution to private IP) Interface VPC Endpoint (private S3 access point) AWS PrivateLink (secure service connection) All traffic remained private without traversing the public internet!\nUnderstanding Your Test Results Connectivity Validation:\nThis successful test confirms:\nDNS Resolution Working:\nOn-prem instance queried Outbound Resolver Query forwarded through VPN to Inbound Resolver Endpoint DNS name resolved to Interface Endpoint private IP Network Routing Functional:\nVPN tunnel carrying traffic between on-prem and cloud VPCs Route tables directing traffic correctly Transit Gateway (if used) routing between VPCs Security Configuration Correct:\nSecurity groups allowing HTTPS traffic to endpoint ENIs Endpoint policy permitting S3 operations IAM permissions on EC2 instance allowing S3 access Interface Endpoint Operational:\nENIs receiving requests on private IPs PrivateLink routing requests to S3 service Responses returning through private path Traffic Flow Diagram:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê DNS Query ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ On-Prem EC2 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u0026gt;‚îÇ Outbound Resolver‚îÇ\r‚îÇ Instance ‚îÇ ‚îÇ (VPC On-prem) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ ‚îÇ\r‚îÇ Data Upload (HTTPS) ‚îÇ DNS Forward\r‚îÇ ‚îÇ\r‚ñº ‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê VPN Tunnel ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ VPN Gateway ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Inbound Resolver ‚îÇ\r‚îÇ EC2 Instance ‚îÇ ‚îÇ (VPC Cloud) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ ‚îÇ\r‚îÇ ‚îÇ DNS Response\r‚ñº ‚îÇ (Private IP)\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\r‚îÇ Transit Gateway ‚îÇ ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\r‚îÇ ‚îÇ\r‚ñº ‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ VPC Cloud ‚îÇ ‚îÇ Private Hosted ‚îÇ\r‚îÇ Route Table ‚îÇ ‚îÇ Zone (Route 53) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Interface Endpoint ENI ‚îÇ\r‚îÇ (Private IP: 10.0.x.x) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚îÇ AWS PrivateLink\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Amazon S3 Service ‚îÇ\r‚îÇ (Backend in AWS Network) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Cleanup for Next Section (Optional) If you want to test again or prepare for the next workshop section:\nIn Session Manager terminal:\n# Remove the local test file to free space rm onprem-data.dat In S3 Console (optional):\nYou can leave the object in S3 or delete it The object will be cleaned up in the final workshop cleanup section Leave Infrastructure Running: Do NOT delete the Interface Endpoint, VPN configuration, or Route 53 Resolvers yet. These will be used in subsequent workshop sections and cleaned up at the end.\nKey Takeaways What You Accomplished:\n‚úÖ Retrieved Interface Endpoint DNS names for API access configuration\n‚úÖ Connected to simulated on-premises server using Session Manager\n‚úÖ Created test data representing corporate files\n‚úÖ Uploaded data to S3 through private Interface Endpoint\n‚úÖ Verified successful transfer in S3 console\n‚úÖ Validated end-to-end hybrid connectivity across multiple AWS services\nEnterprise Architecture Patterns Demonstrated:\nHybrid Cloud Connectivity: VPN-based connection between on-prem and AWS Private Service Access: Using VPC Endpoints to avoid internet routing DNS Resolution: Hybrid DNS architecture with Route 53 Resolver Secure Remote Access: Session Manager for bastion-less connectivity Defense in Depth: Multiple layers (VPN, security groups, endpoint policies) Production Considerations:\nFor real-world deployments, enhance this architecture with:\nAWS Direct Connect instead of VPN for dedicated network connection Multiple VPN tunnels for redundancy and higher availability VPC Flow Logs to monitor and audit traffic patterns CloudWatch metrics for endpoint performance monitoring Restrictive endpoint policies limiting access to specific buckets Private DNS enablement for automatic DNS resolution Multi-region endpoints for disaster recovery scenarios This test validates that your hybrid architecture is production-ready for private S3 access from on-premises systems!\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/3-blogstranslated/",
	"title": "Translated AWS Blogs",
	"tags": [],
	"description": "",
	"content": "This section contains a list of AWS blogs that have been summarized and translated.\nBlog 1 - Learn About Machine Learning on AWS This blog introduces how AWS helps organizations build and operate Machine Learning workloads from basic to advanced levels. It covers the ‚ÄúZero to ML‚Äù journey, MLOps concepts for managing ML lifecycles, and purpose-built infrastructure such as AWS Trainium and Inferentia. The article also highlights real-world case studies from Pinterest and Booking.com, as well as the SageMaker Immersion Day workshop that demonstrates end-to-end ML implementation in near-production environments.\nBlog 2 - Revolutionizing Telecom Revenue Assurance with AWS AI-Driven Framework This blog explains how AWS applies AI/ML and Generative AI to modernize Revenue Assurance (RA) systems in the telecommunications industry. It focuses on a three-layer architecture (Amazon SageMaker, Amazon Bedrock, and Amazon Q) and highlights intelligent revenue-leakage detection, predictive risk analysis, and automated remediation. Real-world use cases include 5G services, partner settlements, and real-time usage reconciliation.\nBlog 3 - Electronic Arts Streamlines Game Patching with AWS This article describes how Electronic Arts (EA) built a Known Version Patching (KVP) system on AWS to optimize game update delivery. The solution leverages Amazon EKS and Amazon EFS to precompute patches on the server side, reducing patch sizes by up to 80% and significantly accelerating download and installation times. This architecture improves both player experience and operational efficiency.\nBlog 4 - Securing Amazon EVS with AWS Network Firewall This blog demonstrates how to secure Amazon Elastic VMware Service (EVS) environments using AWS Network Firewall. The architecture applies a centralized inspection model with AWS Transit Gateway, enabling inspection of both north-south and east-west traffic. The solution provides centralized firewall management, logging, and monitoring, and is well suited for hybrid cloud environments.\nBlog 5 - Building an AI Gateway to Amazon Bedrock with Amazon API Gateway This article presents an AI Gateway architecture that places Amazon API Gateway in front of Amazon Bedrock. The solution allows organizations to enforce access control, quotas, rate limits, and cost governance when using foundation models. It supports multi-tenant environments and enables enterprise-scale generative AI deployments with strong security and observability.\nBlog 6 - Priority-Based Message Processing with Amazon MQ and AWS App Runner This blog explains how to build a priority-based message processing system using Amazon MQ, AWS App Runner, and Amazon DynamoDB. The architecture ensures that critical messages are processed ahead of lower-priority workloads while maintaining reliability and scalability. It supports real-time status updates, retry mechanisms, and dead-letter queues, making it suitable for enterprise-grade, event-driven applications.\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "Configuring Hybrid DNS for Private Endpoint Resolution",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will configure the DNS infrastructure to enable seamless name resolution for S3 API calls from your on-premises environment. This simulates how enterprise networks use conditional DNS forwarding to resolve AWS service endpoints to private IP addresses.\nWhy DNS Configuration Is Critical:\nAWS PrivateLink endpoints use elastic network interfaces (ENIs) with fixed private IP addresses in each Availability Zone. While these IPs remain stable for the endpoint\u0026rsquo;s lifecycle, AWS strongly recommends using DNS resolution rather than hardcoding IPs because:\nDynamic AZ Management: ENIs may be added to new AZs or removed over time Automatic Failover: DNS allows automatic switching between AZ-specific IPs during outages Service Updates: AWS may update endpoint infrastructure requiring new IPs Best Practice: DNS provides abstraction and flexibility for enterprise architectures What You\u0026rsquo;ll Configure:\nDNS Alias Records: Map S3 service domains to Interface Endpoint IPs in Route 53 Private Hosted Zone Resolver Forwarding Rule: Direct on-premises DNS queries for S3 to the cloud VPC End-to-End Testing: Validate DNS resolution and S3 access using the simulated on-premises environment This configuration mimics real-world hybrid DNS architectures where on-premises DNS servers conditionally forward specific domains to AWS for private resolution.\nPart 1: Create DNS Alias Records for Interface Endpoint You\u0026rsquo;ll now create DNS records in the Route 53 Private Hosted Zone that map S3 service names to your Interface Endpoint\u0026rsquo;s private IP addresses.\nUnderstanding the Private Hosted Zone:\nThe CloudFormation stack you deployed earlier created a Private Hosted Zone for s3.us-east-1.amazonaws.com. This zone:\nIs associated only with your VPCs (not public) Overrides public DNS resolution for S3 in your VPCs Allows you to return private IPs instead of public IPs for S3 domains Integrates with Route 53 Resolver for hybrid DNS Step 1: Access Route 53 Hosted Zones\nOpen the Route 53 Management Console (Hosted Zones section) You should see a Private Hosted Zone named s3.us-east-1.amazonaws.com Click on the hosted zone name to view its DNS records Private Hosted Zone Indicator: You\u0026rsquo;ll see \u0026ldquo;Private\u0026rdquo; listed under the Type column, and the Associated VPCs will show which VPCs use this zone for DNS resolution. This ensures only resources in your VPCs receive the private endpoint IPs.\nStep 2: Create Primary Alias Record\nYou\u0026rsquo;ll create an Alias record that points the base S3 domain to your Interface Endpoint.\nInitiate record creation:\nClick the Create record button at the top Configure the first record:\nRecord name: Leave blank (this creates a record for the zone apex: s3.us-east-1.amazonaws.com)\nRecord type: Select A ‚Äì IPv4 address (default)\nAlias configuration:\nToggle the Alias switch to ON (enabled) This tells Route 53 to alias to another AWS resource Route traffic to:\nChoose Alias to VPC endpoint from the dropdown Region:\nSelect US East (N. Virginia) [us-east-1] Choose endpoint:\nPaste the Regional VPC Endpoint DNS name from your text editor This is the DNS name you saved in section 5.4.3 Format: vpce-xxxxxxxxx.s3.us-east-1.vpce.amazonaws.com Alias vs CNAME: Using Alias records (instead of CNAME) provides benefits:\nWorks at the zone apex (bare domain without subdomain) No charge for Alias queries to AWS resources Better performance with AWS service integration Automatic health checking and failover support Step 3: Create Wildcard Alias Record\nNow create a second record to handle all subdomains of the S3 service (e.g., bucket.s3.us-east-1.amazonaws.com).\nAdd another record:\nClick Add another record button (do not click Create yet) Configure the wildcard record:\nRecord name: Enter * (asterisk - this creates a wildcard)\nRecord type: Keep A ‚Äì IPv4 address\nAlias configuration:\nToggle Alias to ON Route traffic to:\nChoose Alias to VPC endpoint Region:\nSelect US East (N. Virginia) [us-east-1] Choose endpoint:\nPaste the same Regional VPC Endpoint DNS name Create both records:\nClick Create records button Both records will be created simultaneously Why Two Records?\nThe two DNS records serve different purposes:\nApex Record (s3.us-east-1.amazonaws.com):\nHandles queries to the base S3 regional endpoint Used by commands like: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Wildcard Record (*.s3.us-east-1.amazonaws.com):\nHandles queries with bucket names in the subdomain Matches: bucket.s3.us-east-1.amazonaws.com, my-bucket.s3.us-east-1.amazonaws.com, etc. Used by path-style S3 URLs Step 4: Verify Record Creation\nAfter creation, you should see both records in the hosted zone:\nVerify:\nTwo new A records (apex and wildcard) Both show Type: A - Alias Value points to your VPC endpoint DNS Records Configured! These records ensure that when systems query for S3 domain names, they receive the private IP addresses of your Interface Endpoint ENIs instead of public S3 IPs.\nPart 2: Create Route 53 Resolver Forwarding Rule Now you\u0026rsquo;ll configure conditional DNS forwarding from the on-premises VPC to the cloud VPC. This simulates how on-premises DNS servers forward specific domains to AWS for resolution.\nUnderstanding Resolver Forwarding Rules:\nRoute 53 Resolver Forwarding Rules enable:\nConditional forwarding: Only specific domains are forwarded (e.g., s3.us-east-1.amazonaws.com) Hybrid DNS: On-premises systems query their local DNS, which forwards to AWS Centralized management: Rules define which domains resolve through AWS Bidirectional resolution: Can forward both from AWS to on-prem and vice versa In this workshop:\nOn-prem EC2 queries the Outbound Resolver in VPC On-prem Outbound Resolver forwards S3 queries through VPN to Inbound Resolver in VPC Cloud Inbound Resolver queries the Private Hosted Zone Response with private IPs travels back to on-prem Step 1: Retrieve Inbound Resolver IP Addresses\nFirst, you need the IP addresses where the Inbound Resolver is listening in VPC Cloud.\nNavigate to Inbound Endpoints:\nFrom Route 53 console, click Resolver in left sidebar Click Inbound endpoints Access endpoint details:\nClick on the Endpoint ID of the inbound endpoint CloudFormation created this with a name like VPCCloudInboundEndpoint Copy IP addresses: In the endpoint details, scroll to the IP addresses section You\u0026rsquo;ll see 2 IP addresses (one per AZ) Copy both IPs to your text editor Format: 10.0.x.x (within VPC Cloud CIDR) Multi-AZ Resolver: The Inbound Resolver has IPs in multiple AZs for high availability. The forwarding rule will use both, automatically failing over if one AZ becomes unavailable.\nStep 2: Create Forwarding Rule\nNavigate to Rules: In Route 53 console, click Resolver \u0026gt; Rules in left sidebar Click Create rule button Configure rule basics:\nName: Enter S3-PrivateEndpoint-ForwardingRule Description (optional): \u0026ldquo;Forward S3 DNS queries to VPC Cloud for private endpoint resolution\u0026rdquo; Rule type: Select Forward (not System) Domain name: Enter s3.us-east-1.amazonaws.com This domain name tells the rule: \u0026ldquo;Forward any DNS queries for this domain and its subdomains.\u0026rdquo;\nStep 3: Associate VPC and Outbound Endpoint\nVPC Configuration:\nVPCs to associate this rule with:\nSelect VPC On-prem from the dropdown This applies the rule to DNS queries originating in the on-premises VPC Outbound endpoint:\nSelect VPCOnpremOutboundEndpoint This is the resolver endpoint that forwards queries from VPC On-prem Outbound Endpoint Role: The Outbound Resolver in VPC On-prem receives DNS queries from EC2 instances and forwards them through the VPN tunnel to the target IP addresses (the Inbound Resolver in VPC Cloud).\nStep 4: Specify Target IP Addresses\nAdd target IPs:\nTarget IP addresses: This is where forwarded queries will be sent Click Add target IP address IP address: Paste the first Inbound Resolver IP from your text editor Click Add target IP address again IP address: Paste the second Inbound Resolver IP Both IPs should now be listed Create the rule:\nClick Submit button Understanding the Target IPs:\nThese are the Inbound Resolver IPs in VPC Cloud:\nThey listen for DNS queries coming through the VPN They query the Private Hosted Zone for answers They return results back through the VPN to the Outbound Resolver Step 5: Verify Rule Creation\nAfter creation, you should see your new forwarding rule:\nCheck that:\nStatus shows Active Type shows Forward Domain name is s3.us-east-1.amazonaws.com VPC shows VPC On-prem DNS Forwarding Configured! Queries from VPC On-prem for s3.us-east-1.amazonaws.com now forward through the Outbound Resolver, across the VPN tunnel, to the Inbound Resolver, and finally resolve through the Private Hosted Zone.\nPart 3: Test On-Premises DNS Resolution Now validate that the entire DNS resolution chain is working correctly.\nStep 1: Connect to On-Premises Instance\nOpen AWS Systems Manager \u0026gt; Session Manager Click Start session Select the Test-Interface-Endpoint EC2 instance (in VPC On-prem) Click Start session Step 2: Test DNS Resolution with Dig Command\nIn the Session Manager terminal, run:\ndig +short s3.us-east-1.amazonaws.com Expected Output:\nYou should see 2 IP addresses returned:\n10.0.1.100\r10.0.2.100 (Your actual IPs will differ but should be in the VPC Cloud CIDR range)\nImportant Distinction: The IP addresses returned are the VPC Interface Endpoint ENI IPs, NOT the Inbound Resolver IPs you configured. Both sets of IPs are in the VPC Cloud CIDR block, but they serve different purposes:\nInbound Resolver IPs: Handle DNS queries (not returned in dig results) Interface Endpoint IPs: Where S3 API traffic is sent (returned by DNS queries) What Just Happened:\nThe dig command triggered this DNS resolution flow:\nEC2 On-prem ‚Üí Outbound Resolver (VPC On-prem) ‚Üí VPN Tunnel ‚Üí Inbound Resolver (VPC Cloud) ‚Üí Private Hosted Zone ‚Üí Alias Record ‚Üí Interface Endpoint IPs ‚Üí Response back to EC2 Step 3: Verify IPs Match Interface Endpoint\nCross-check the IPs returned by dig against the actual Interface Endpoint:\nOpen VPC Console:\nNavigate to VPC \u0026gt; Endpoints Click on your S3 Interface Endpoint Check Subnets tab:\nClick the Subnets tab You\u0026rsquo;ll see ENI IDs and their associated private IPs Verify these IPs match those returned by the dig command The IPs should match perfectly, confirming DNS resolution is working correctly.\nDNS Resolution Validated! Your on-premises environment is successfully resolving S3 domain names to private Interface Endpoint IPs through the hybrid DNS architecture.\nStep 4: Test S3 API Access via DNS\nNow test that applications can use the standard S3 endpoint URL and automatically connect through the private endpoint.\nIn the Session Manager terminal, run:\naws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Expected Output:\nYou should see a list of your S3 buckets, including the test bucket created earlier.\nKey Observation:\nNotice you\u0026rsquo;re now using the standard S3 endpoint URL (https://s3.us-east-1.amazonaws.com), not the VPC endpoint-specific URL you used in section 5.4.3. This is possible because:\nDNS resolves s3.us-east-1.amazonaws.com to Interface Endpoint private IPs HTTPS traffic routes through VPN to the Interface Endpoint Applications don\u0026rsquo;t need to be aware of the private endpoint configuration Standard AWS SDKs and CLI work without modification This demonstrates the power of DNS-based private endpoint resolution!\nProduction Benefit: In enterprise environments, applications can use standard AWS service endpoints in their code. The DNS infrastructure automatically directs traffic through private endpoints when running on-premises or in VPCs, and through internet when running elsewhere (with appropriate routing).\nStep 5: Clean Up Session\nTerminate your Session Manager session by typing:\nexit Or close the browser tab\nUnderstanding Your Hybrid DNS Architecture Complete Traffic Flow:\nWhen an on-premises application accesses S3, this is the complete flow:\nDNS Resolution Path:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ On-Prem Application ‚îÇ\r‚îÇ Queries: s3.us-... ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ EC2 Instance ‚îÇ\r‚îÇ DNS Resolver Config ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Outbound Resolver ‚îÇ\r‚îÇ (VPC On-prem) ‚îÇ\r‚îÇ Forwarding Rule: ‚îÇ\r‚îÇ s3.us-east-1.* ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚îÇ VPN Tunnel\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Inbound Resolver ‚îÇ\r‚îÇ (VPC Cloud) ‚îÇ\r‚îÇ IPs: 10.0.x.x ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Private Hosted Zone ‚îÇ\r‚îÇ s3.us-east-1.*.com ‚îÇ\r‚îÇ Alias Records ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Returns: 10.0.1.100 ‚îÇ\r‚îÇ 10.0.2.100 ‚îÇ\r‚îÇ (Endpoint ENI IPs) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Data Path (After DNS Resolution):\nOn-Prem Application ‚Üí Interface Endpoint Private IP ‚Üí VPN Tunnel ‚Üí VPC Cloud ‚Üí Interface Endpoint ENI ‚Üí AWS PrivateLink ‚Üí S3 Service Key Accomplishments What You\u0026rsquo;ve Built:\n‚úÖ DNS Alias Records: Mapped S3 domains to Interface Endpoint private IPs\n‚úÖ Conditional Forwarding: Configured forwarding rule for S3 domain from on-prem to cloud\n‚úÖ Hybrid DNS Resolution: Enabled seamless DNS resolution across VPN\n‚úÖ Transparent S3 Access: Applications use standard S3 endpoints with private routing\n‚úÖ High Availability: Multi-AZ DNS and endpoint configuration for fault tolerance\nEnterprise Architecture Patterns:\nConditional DNS Forwarding: Split-brain DNS with domain-specific forwarding Private Hosted Zones: Override public DNS for AWS services with private IPs Alias Records: Efficient DNS routing to AWS resources Resolver Endpoints: Hybrid DNS bridge between on-prem and cloud Multi-AZ Resilience: Automatic failover for DNS and endpoints Production Enhancements:\nFor real-world deployments, consider:\nDNS caching strategies to reduce resolver query load Monitoring CloudWatch metrics for Resolver query counts and latency Multiple forwarding rules for different AWS services (EC2, RDS, etc.) DNS failback to public resolution if private endpoints are unavailable Integration with on-premises DNS servers (BIND, Active Directory, etc.) DNS query logging for security auditing and troubleshooting Route 53 Resolver Query Logging enabled for compliance This hybrid DNS architecture provides enterprise-grade private connectivity to AWS services while maintaining standard application code and AWS SDK usage!\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section list and describe the events you participated in during your internship or work. Provide details per event such as:\nEvent name Date/time Location Your role (attendee, organizer, speaker, etc.) Short description and main activities Outcomes or lessons learned Event 1 Event name: GenAI-powered App-DB Modernization workshop\nTime: 09:00 13/08/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu St., Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event name: GenAI-powered App-DB Modernization workshop\nTime: 09:00 13/08/2025\nLocation: 26th floor, Bitexco Tower, 02 Hai Trieu St., Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.5-policy/",
	"title": "Implementing Fine-Grained Access Control with VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will implement security best practices by applying restrictive VPC endpoint policies to control which S3 buckets can be accessed through your Gateway VPC Endpoint. This demonstrates the principle of least privilege in cloud architecture.\nUnderstanding VPC Endpoint Policies:\nVPC endpoint policies are IAM resource policies that provide granular access control for traffic flowing through VPC endpoints. They work as an additional security layer:\nDefault Behavior: When you create an endpoint without a custom policy, AWS applies a permissive default policy allowing full access to all resources of the target service Policy Type: JSON-based IAM resource policies attached directly to the endpoint Enforcement Point: Evaluated when traffic passes through the endpoint (not on the AWS service side) Combined Evaluation: Works in conjunction with IAM user/role policies and S3 bucket policies (all must allow an action) Scope: Can restrict access by resource (specific buckets), principal (specific IAM identities), or action (specific S3 operations) Why Use Endpoint Policies?\nEndpoint policies are critical for enterprise security architectures:\nDefense in Depth: Additional security layer beyond IAM and resource policies Network-Level Control: Restrict what can be accessed through specific network paths Compliance Requirements: Meet regulatory requirements for data access controls Prevent Data Exfiltration: Limit which buckets can be accessed from VPC resources Cost Optimization: Restrict access to authorized resources only, preventing misuse Workshop Scenario:\nYou\u0026rsquo;ll create a restrictive endpoint policy that:\nAllows access to a specific S3 bucket only Blocks access to all other S3 buckets Demonstrates how endpoint policies enforce access controls Part 1: Establish Baseline Connectivity Before applying a restrictive policy, verify current access to demonstrate the before/after comparison.\nStep 1: Connect to Test Instance\nOpen AWS Systems Manager \u0026gt; Session Manager Click Start session Select the EC2 instance named Test-Gateway-Endpoint This instance is in the VPC with the Gateway Endpoint It routes S3 traffic through the Gateway Endpoint via route table configuration Click Start session Step 2: Verify Access to Original S3 Bucket\nIn the Session Manager terminal, list the contents of the bucket you created in section 5.3:\naws s3 ls s3://\u0026lt;your-bucket-name\u0026gt; Replace \u0026lt;your-bucket-name\u0026gt; with your actual bucket name (e.g., my-endpoint-test-bucket-123456).\nExpected Output:\nYou should see the two 1GB test files uploaded in earlier sections:\n2024-12-07 10:30:15 1073741824 testfile.txt\r2024-12-07 11:45:22 1073741824 onprem-data.dat Current Access: This works because the Gateway Endpoint currently has the default policy allowing full access (\u0026quot;*\u0026quot;) to all S3 resources. You\u0026rsquo;re about to change this to demonstrate policy-based restrictions.\nPart 2: Create Second S3 Bucket for Policy Testing You\u0026rsquo;ll create a second bucket to demonstrate selective access control.\nStep 1: Navigate to S3 Console\nOpen the Amazon S3 Console Click Create bucket Step 2: Configure New Bucket\nBucket name:\nFollow your existing naming pattern, but append -restricted Example: If your first bucket is my-endpoint-test-bucket-123456, name this one my-endpoint-test-bucket-123456-restricted Must be globally unique across all AWS accounts Region:\nEnsure it\u0026rsquo;s US East (N. Virginia) us-east-1 (same as your VPC and first bucket) Other settings:\nLeave all other settings at default values Default encryption: Enabled (SSE-S3) Block Public Access: Enabled (recommended) Versioning: Disabled (for workshop simplicity) Create bucket:\nScroll to bottom and click Create bucket Step 3: Verify Bucket Creation\nYou should see a success message and the new bucket listed in the S3 console.\nNaming Strategy: Using a consistent naming pattern (original name + suffix) makes it easy to identify related resources and simplifies policy ARN construction. In production, use naming conventions that reflect environment, application, and purpose.\nPart 3: Apply Restrictive Endpoint Policy Now you\u0026rsquo;ll modify the Gateway Endpoint policy to allow access only to the new bucket.\nStep 1: Navigate to VPC Endpoints\nOpen the VPC Console Click Endpoints in the left sidebar Locate and select the Gateway Endpoint you created in section 5.3 Type should show \u0026ldquo;Gateway\u0026rdquo; Service name should be com.amazonaws.us-east-1.s3 Step 2: Access Endpoint Policy Editor\nWith the endpoint selected, click the Policy tab in the bottom details pane You\u0026rsquo;ll see the current policy (default allows all) Click Edit policy button Current Default Policy:\nThe existing policy looks like this (allowing unrestricted access):\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 3: Apply Restrictive Policy\nClear the existing policy in the policy editor\nCopy and paste the following restrictive policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;RestrictToSpecificBucketPolicy\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAccessToRestrictedBucketOnly\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::YOUR-RESTRICTED-BUCKET-NAME\u0026#34;, \u0026#34;arn:aws:s3:::YOUR-RESTRICTED-BUCKET-NAME/*\u0026#34; ] } ] } Replace the ARN placeholders:\nFind YOUR-RESTRICTED-BUCKET-NAME (appears twice) Replace with your actual second bucket name (the one ending in -restricted) Example result: \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::my-endpoint-test-bucket-123456-restricted\u0026#34;, \u0026#34;arn:aws:s3:::my-endpoint-test-bucket-123456-restricted/*\u0026#34; ] Save the policy:\nClick Save button Understanding the Policy:\nLet\u0026rsquo;s break down each component:\nVersion: IAM policy language version (always use \u0026ldquo;2012-10-17\u0026rdquo;) Id: Optional identifier for the policy Statement: Array of policy statements (can have multiple) Sid: Statement ID (descriptive label) Effect: Allow (grants permissions) or Deny (explicitly denies) Principal: \u0026quot;*\u0026quot; means any IAM identity (still subject to IAM policies) Action: s3:* allows all S3 operations (GetObject, PutObject, ListBucket, etc.) Resource: Two ARNs: First ARN: The bucket itself (for operations like ListBucket) Second ARN with /*: All objects within the bucket (for GetObject, PutObject, etc.) Policy Applied! The Gateway Endpoint now enforces that only the restricted bucket can be accessed through it. All attempts to access other buckets (including your original bucket) will be denied.\nPolicy Evaluation Logic:\nWhen an EC2 instance tries to access S3 through the endpoint:\n1. IAM policy on EC2 instance role: Must Allow\r2. VPC Endpoint policy: Must Allow ‚Üê We just restricted this\r3. S3 bucket policy: Must Allow (or not explicitly Deny)\r4. S3 ACLs: Must Allow (or not explicitly Deny)\rOnly if ALL checks pass ‚Üí Access granted Part 4: Test Policy Enforcement Now validate that the endpoint policy is enforcing access restrictions.\nStep 1: Test Access to Original Bucket (Should Fail)\nIn your still-open Session Manager terminal on Test-Gateway-Endpoint, try to list the original bucket:\naws s3 ls s3://\u0026lt;your-original-bucket-name\u0026gt; Expected Output:\nYou should receive an Access Denied error:\nAn error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied Why Access Denied? The VPC endpoint policy no longer includes the original bucket in its allowed resources. Even though your EC2 instance IAM role has S3 permissions, the network path (through the endpoint) blocks access.\nStep 2: Prepare Test Data\nCreate a test file to upload to the restricted bucket:\nNavigate to home directory:\ncd ~ Create a 1GB test file:\nfallocate -l 1G policy-test-file.dat Step 3: Test Access to Restricted Bucket (Should Succeed)\nUpload the file to the restricted bucket:\naws s3 cp policy-test-file.dat s3://\u0026lt;your-restricted-bucket-name\u0026gt; Replace \u0026lt;your-restricted-bucket-name\u0026gt; with your second bucket name.\nExpected Output:\nThe upload should succeed:\nupload: ./policy-test-file.dat to s3://my-bucket-restricted/policy-test-file.dat\rCompleted 1.0 GiB/1.0 GiB Step 4: Verify File Upload in S3 Console\nGo back to the S3 Console Click on your restricted bucket You should see the policy-test-file.dat object Policy Enforcement Confirmed! The endpoint policy successfully:\n‚úÖ Allowed access to the restricted bucket ‚ùå Blocked access to the original bucket This demonstrates fine-grained network-level access control.\nStep 5: Confirm Original Bucket Is Still Blocked\nAs a final test, try to upload to the original bucket (should fail):\naws s3 cp policy-test-file.dat s3://\u0026lt;your-original-bucket-name\u0026gt; Expected Output:\nAccess denied error:\nupload failed: ./policy-test-file.dat to s3://my-original-bucket/policy-test-file.dat\rAn error occurred (AccessDenied) when calling the PutObject operation: Access Denied This confirms the endpoint policy is consistently enforcing restrictions.\nUnderstanding Policy Behavior Access Patterns Summary:\nOperation Target Result Reason List objects Original bucket ‚ùå Denied Not in endpoint policy Resource list Upload file Original bucket ‚ùå Denied Not in endpoint policy Resource list List objects Restricted bucket ‚úÖ Allowed Explicitly allowed in endpoint policy Upload file Restricted bucket ‚úÖ Allowed Explicitly allowed in endpoint policy How Traffic Flows:\nWhen EC2 accesses S3 through the Gateway Endpoint with a restrictive policy:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ EC2 Instance ‚îÇ\r‚îÇ Test-Gateway-EP ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚îÇ S3 API Request\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Route Table ‚îÇ\r‚îÇ Directs S3 traffic ‚îÇ\r‚îÇ to Gateway Endpoint‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Gateway VPC Endpoint ‚îÇ\r‚îÇ Evaluates Endpoint Policy ‚îÇ\r‚îÇ ‚îÇ\r‚îÇ IF bucket IN allowed list: ‚îÇ\r‚îÇ ‚Üí Forward to S3 ‚îÇ\r‚îÇ ELSE: ‚îÇ\r‚îÇ ‚Üí Return Access Denied ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\r‚îÇ\r‚îÇ (If allowed)\r‚ñº\r‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\r‚îÇ Amazon S3 Service ‚îÇ\r‚îÇ (Backend) ‚îÇ\r‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Key Observations:\nNetwork-Level Enforcement: The endpoint policy blocks traffic before it even reaches S3 Independent of IAM: Even if IAM policies allow access, endpoint policy can block it All-or-Nothing per Bucket: You cannot selectively allow only certain operations (e.g., read-only); the policy applies to all allowed actions Principal Agnostic: With \u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;, the restriction applies to all identities using the endpoint Production Use Cases for Endpoint Policies Enterprise Scenarios:\nData Residency Compliance:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::eu-compliant-data-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:ExistingObjectTag/DataClassification\u0026#34;: \u0026#34;EUPersonalData\u0026#34; } } }] } Prevent Data Exfiltration:\nWhitelist only approved corporate buckets Block access to personal or external buckets Audit endpoint policy changes with CloudTrail Multi-Tenant Environments:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::tenant-a-data/*\u0026#34;, \u0026#34;arn:aws:s3:::shared-resources/*\u0026#34; ] }] } Development vs Production Isolation:\nDev VPC endpoint: Access only to dev/test buckets Prod VPC endpoint: Access only to production buckets Prevents accidental cross-environment access Advanced Policy Patterns:\nCondition-Based Access:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::sensitive-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceVpce\u0026#34;: \u0026#34;vpce-xxxxxxxx\u0026#34; } } }] } Read-Only Access:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::readonly-bucket\u0026#34;, \u0026#34;arn:aws:s3:::readonly-bucket/*\u0026#34; ] }] } Key Takeaways What You Accomplished:\n‚úÖ Baseline Testing: Verified unrestricted access with default endpoint policy\n‚úÖ Bucket Creation: Created second S3 bucket for policy testing\n‚úÖ Policy Application: Applied restrictive endpoint policy limiting access to specific bucket\n‚úÖ Enforcement Validation: Confirmed policy blocks unauthorized buckets and allows authorized bucket\n‚úÖ Security Best Practice: Demonstrated principle of least privilege at network level\nSecurity Principles Demonstrated:\nDefense in Depth: Endpoint policies add network-level control layer Least Privilege: Grant only necessary access, not broad permissions Explicit Allow: Whitelist approach (specify what\u0026rsquo;s allowed, block everything else) Network Segmentation: Control data flow at VPC endpoint level Auditability: Policy changes logged in CloudTrail for compliance Production Recommendations:\nVersion Control: Store endpoint policies in Git with code review process Policy Testing: Test policies in dev/staging before production deployment Monitoring: Set up CloudWatch alarms for AccessDenied errors indicating policy violations Documentation: Maintain clear documentation of which buckets are accessible through which endpoints Regular Review: Audit endpoint policies quarterly to remove unnecessary access Condition Keys: Use advanced IAM condition keys for time-based, IP-based, or tag-based restrictions Deny Statements: Consider explicit Deny statements for high-security requirements This section demonstrated how VPC endpoint policies provide critical access control for enterprise cloud architectures, enabling you to enforce organizational security policies at the network infrastructure level!\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/5-workshop/5.6-cleanup/",
	"title": "Resource Cleanup and Workshop Completion",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nCongratulations on completing this comprehensive VPC Endpoints workshop! You\u0026rsquo;ve successfully explored enterprise-grade architectures for private AWS service connectivity.\nWhat You\u0026rsquo;ve Accomplished:\nThroughout this workshop, you implemented multiple AWS networking best practices:\n‚úÖ Gateway VPC Endpoint Architecture:\nCreated a Gateway Endpoint for S3 access from VPC resources Configured route table entries for private S3 connectivity Eliminated Internet Gateway dependencies for S3 traffic Implemented network-level access controls with endpoint policies ‚úÖ Interface VPC Endpoint with Hybrid Connectivity:\nDeployed Interface Endpoints with multi-AZ redundancy Configured Route 53 Resolver for hybrid DNS resolution Established VPN-based connectivity simulating on-premises access Created Private Hosted Zones for endpoint DNS resolution Implemented conditional DNS forwarding rules ‚úÖ Security Best Practices:\nApplied restrictive VPC endpoint policies for least privilege access Configured security groups for endpoint traffic control Demonstrated defense-in-depth with multiple policy layers Validated policy enforcement through practical testing ‚úÖ Production-Ready Skills:\nMulti-AZ deployment patterns for high availability Hybrid cloud DNS architecture design Private connectivity without internet exposure Cost optimization through VPC endpoint usage Why Resource Cleanup Is Critical Proper resource cleanup is essential for:\nCost Avoidance: Prevent ongoing charges for unused resources (VPC endpoints, Route 53 Resolver endpoints, CloudFormation stacks) Account Hygiene: Maintain a clean AWS environment for future projects Security Best Practice: Remove test infrastructure that may have permissive configurations Service Limits: Free up service quotas for production workloads Learning Reinforcement: Understanding deletion dependencies teaches AWS resource relationships Estimated Cleanup Time: 10-15 minutes if following the recommended order. Deletion dependencies require sequential processing for some resources.\nCleanup Order and Dependencies AWS resources have dependencies that require deletion in a specific order. Follow this sequence to avoid errors:\nDeletion Order:\n1. Route 53 DNS Records (depend on Hosted Zone)\r2. Route 53 Resolver Rules (depend on Resolver Endpoints)\r3. Route 53 Hosted Zones (depend on VPCs)\r4. VPC Endpoints (depend on VPCs and Security Groups)\r5. S3 Bucket Objects (must be empty before bucket deletion)\r6. S3 Buckets (depend on being empty)\r7. CloudFormation Stacks (delete VPC infrastructure last) Step-by-Step Cleanup Process Step 1: Delete Route 53 Private Hosted Zone DNS Records\nBefore deleting the hosted zone, remove the custom DNS records you created.\nNavigate to Route 53:\nOpen the Route 53 Console Click Hosted Zones in the left sidebar Access the Private Hosted Zone:\nClick on the hosted zone named s3.us-east-1.amazonaws.com You\u0026rsquo;ll see the DNS records including the Alias records you created Delete Custom DNS Records:\nSelect the Alias A records you created (apex and wildcard records pointing to the Interface Endpoint) Do NOT select the default NS and SOA records (required for hosted zone) Click Delete records Confirm deletion Delete the Hosted Zone:\nGo back to Hosted Zones list Select the s3.us-east-1.amazonaws.com zone Click Delete hosted zone Type delete in the confirmation box to confirm Click Delete Hosted Zone Deletion: You cannot delete a hosted zone that still has DNS records (except the mandatory NS and SOA records). Ensure you\u0026rsquo;ve removed all custom records first.\nStep 2: Remove Route 53 Resolver Forwarding Rule\nThe resolver rule must be disassociated from VPCs before deletion.\nNavigate to Resolver Rules:\nIn Route 53 console, click Resolver \u0026gt; Rules in the left sidebar Locate Your Forwarding Rule:\nFind the rule you created (e.g., S3-PrivateEndpoint-ForwardingRule) The rule forwards s3.us-east-1.amazonaws.com queries Disassociate from VPC:\nSelect the rule Click View details Click the VPC associations tab Select the associated VPC On-prem Click Disassociate Confirm the disassociation Delete the Rule:\nOnce disassociated, go back to the Rules list Select the rule Click Delete Confirm deletion Rule Dependencies: Resolver rules must be disassociated from all VPCs before deletion. Attempting to delete an associated rule will result in an error.\nStep 3: Delete VPC Endpoints\nRemove both the Gateway and Interface endpoints you created.\nNavigate to VPC Endpoints:\nOpen the VPC Console Click Endpoints in the left sidebar Delete Interface Endpoint:\nSelect the Interface Endpoint for S3 (the one in VPC Cloud) Endpoint type should show \u0026ldquo;Interface\u0026rdquo; Click Actions \u0026gt; Delete VPC endpoints Type delete to confirm Click Delete Delete Gateway Endpoint:\nSelect the Gateway Endpoint for S3 (also in VPC Cloud) Endpoint type should show \u0026ldquo;Gateway\u0026rdquo; Click Actions \u0026gt; Delete VPC endpoints Type delete to confirm Click Delete Immediate Deletion: VPC endpoints delete quickly (usually within 1-2 minutes). You can proceed to the next step while they\u0026rsquo;re being removed.\nStep 4: Empty and Delete S3 Buckets\nS3 buckets must be completely empty before they can be deleted.\nNavigate to S3 Console:\nOpen the S3 Console Empty the First Bucket:\nSelect your original test bucket (e.g., my-endpoint-test-bucket-123456) Click Empty Type permanently delete in the confirmation box Click Empty Wait for the empty operation to complete Delete the First Bucket:\nWith the bucket still selected, click Delete Type the exact bucket name to confirm Click Delete bucket Repeat for Second Bucket:\nSelect your restricted bucket (e.g., my-endpoint-test-bucket-123456-restricted) Click Empty, confirm with permanently delete Click Delete, type the bucket name, confirm deletion Data Loss Warning: Emptying and deleting S3 buckets permanently removes all objects. Ensure you don\u0026rsquo;t need any test data before proceeding. There is no recovery after deletion.\nStep 5: Delete CloudFormation Stacks\nCloudFormation stacks will automatically delete all resources they created (VPCs, subnets, EC2 instances, Route 53 Resolver endpoints, etc.).\nNavigate to CloudFormation:\nOpen the CloudFormation Console Delete PLOnpremSetup Stack:\nSelect the PLOnpremSetup stack This stack contains the DNS infrastructure (Resolver endpoints, Private Hosted Zone association) Click Delete Confirm deletion Deletion takes 3-5 minutes Wait for First Stack Deletion:\nMonitor the stack status - it will show \u0026ldquo;DELETE_IN_PROGRESS\u0026rdquo; Wait until it reaches \u0026ldquo;DELETE_COMPLETE\u0026rdquo; or disappears from the list You can view deletion events in the Events tab Delete PLCloudSetup Stack:\nSelect the PLCloudSetup stack This stack contains the VPC infrastructure (VPCs, subnets, EC2 instances, VPN gateway, etc.) Click Delete Confirm deletion Deletion takes 5-10 minutes due to the number of resources Stack Deletion Process: CloudFormation automatically handles resource dependencies within the stack, deleting resources in the correct order. You\u0026rsquo;ll see individual resources being deleted in the Events tab.\nWhat Gets Deleted by CloudFormation Stacks:\nPLOnpremSetup Stack Removes:\nRoute 53 Inbound Resolver Endpoint (in VPC Cloud) Route 53 Outbound Resolver Endpoint (in VPC On-prem) Associated elastic network interfaces IAM roles and policies for Resolver PLCloudSetup Stack Removes:\nVPC Cloud and VPC On-prem Public and private subnets in multiple AZs Internet Gateways and NAT Gateways Route tables and route table associations Security groups EC2 instances (Test-Gateway-Endpoint, Test-Interface-Endpoint, VPN Gateway) Transit Gateway and attachments VPN connections Elastic IPs Verification of Complete Cleanup After completing all deletion steps, verify that resources are fully removed:\nChecklist:\nRoute 53:\nNo private hosted zone for s3.us-east-1.amazonaws.com No resolver rules for S3 domain Inbound and Outbound Resolver endpoints gone VPC:\nNo endpoints in the Endpoints list VPC Cloud and VPC On-prem removed (after CloudFormation deletion) No custom security groups related to the workshop S3:\nBoth test buckets deleted No objects remaining from the workshop CloudFormation:\nBoth stacks show \u0026ldquo;DELETE_COMPLETE\u0026rdquo; or are no longer listed No failed deletion states EC2:\nNo instances named Test-Gateway-Endpoint or Test-Interface-Endpoint No VPN Gateway EC2 instance Cleanup Complete! If all items in the checklist are verified, you\u0026rsquo;ve successfully removed all workshop resources. Your AWS account is now clean and you won\u0026rsquo;t incur ongoing charges for this workshop.\nTroubleshooting Cleanup Issues Common Deletion Problems:\nIssue: Hosted Zone Won\u0026rsquo;t Delete\nCause: Custom DNS records still present Solution: Delete all A records except NS and SOA records, then retry Issue: Resolver Rule Won\u0026rsquo;t Delete\nCause: Still associated with VPC Solution: Disassociate from all VPCs first, then delete Issue: VPC Endpoint Stuck Deleting\nCause: Active connections or route table associations Solution: Wait 5 minutes and check again; should auto-resolve Issue: S3 Bucket Won\u0026rsquo;t Delete\nCause: Objects still in bucket or versioning enabled Solution: Empty bucket completely, disable versioning if enabled Issue: CloudFormation Stack Deletion Failed\nCause: Manual resource deletion or dependencies outside stack Solution: Check Events tab for specific resource errors; may need to manually delete blocking resources Cost Optimization Lessons Workshop Resource Costs (if left running):\nApproximate hourly costs for reference:\nInterface VPC Endpoint: ~$0.01/hour + $0.01/GB data processed Route 53 Resolver Endpoints: ~$0.125/hour per endpoint (2 endpoints = $0.25/hour) NAT Gateway: ~$0.045/hour + $0.045/GB data processed EC2 instances (t3.micro): ~$0.0104/hour each (3 instances = $0.03/hour) Monthly cost if not deleted: ~$200-250\nKey Takeaway: Always delete test resources promptly to avoid unnecessary charges!\nWhat You\u0026rsquo;ve Learned Technical Skills:\nVPC Endpoint architecture patterns (Gateway vs Interface) Hybrid DNS configuration with Route 53 Resolver VPN-based hybrid connectivity Multi-AZ high availability patterns Security policy implementation and testing CloudFormation infrastructure as code AWS Services Mastered:\nAmazon VPC (Endpoints, Subnets, Route Tables, Security Groups) Amazon S3 (Bucket policies, private access) Route 53 (Private Hosted Zones, Resolver endpoints, Forwarding rules) AWS PrivateLink (Interface Endpoints) CloudFormation (Stack management) Systems Manager (Session Manager) IAM (Policies, roles, endpoint policies) Enterprise Architecture Patterns:\nDefense in depth security Least privilege access control Network segmentation Private connectivity without internet exposure Hybrid cloud integration High availability design Next Steps for Continued Learning Advanced Topics to Explore:\nAWS Direct Connect: Replace VPN with dedicated network connection AWS Transit Gateway: Centralized hub for multi-VPC connectivity VPC Peering: Alternative connectivity pattern between VPCs PrivateLink for Custom Services: Expose your own services via PrivateLink Multi-Region Endpoints: Disaster recovery with cross-region endpoints AWS Network Firewall: Advanced traffic inspection and filtering VPC Flow Logs: Network traffic analysis and security monitoring Recommended AWS Documentation:\nVPC Endpoints Route 53 Resolver AWS PrivateLink VPC Endpoint Policies Practice Exercises:\nImplement similar patterns for other AWS services (RDS, DynamoDB, EC2 API) Design multi-region endpoint architectures Integrate with AWS Transit Gateway for enterprise hub-and-spoke Implement comprehensive logging and monitoring Thank you for completing this workshop! You now have practical experience with AWS private connectivity patterns essential for enterprise cloud architectures. Apply these skills to build secure, scalable, and cost-effective solutions in your real-world projects!\n"
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Co., Ltd. from 8/9/2025 to 12/9/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment. I participated in First Cloud Journey, through which I improved my skills in Communication, financial management, more knowledge about AWS and Cloud: translation.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "\r‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://thinhpxse.github.io/fcj-workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]